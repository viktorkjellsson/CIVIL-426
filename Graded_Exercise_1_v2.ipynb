{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "42fd2307-c401-4864-8273-38bcbd5b8b42",
   "metadata": {},
   "source": [
    "# Graded Exercise 1: Extraction of Discriminative Features for Anomaly Detection on Acoustic Data\n",
    "\n",
    "- **Course**: [CIVIL-426 - Machine Learning for Predictive Maintenance](https://edu.epfl.ch/coursebook/en/machine-learning-for-predictive-maintenance-applications-CIVIL-426)\n",
    "- **Start Date**: 2024.09.19 at 10h15\n",
    "- **Due Date**: 2024.10.02 at 23h59\n",
    "- **Student 0**:\n",
    "    - First and Last name: `FILL HERE`\n",
    "    - SCIPER: `FILL HERE`\n",
    "- **Student 1**:\n",
    "    - First and Last name: `FILL HERE`\n",
    "    - SCIPER: `FILL HERE`\n",
    "\n",
    "## Introduction\n",
    "\n",
    "This notebook is divided in 2 parts:\n",
    "- [Part 0](#part-0-demonstration): Demonstration on a toy dataset\n",
    "\n",
    "This part shows how to extract discriminative features for anomaly detection on a toy \n",
    "dataset. This part of the notebook runs without any issues on a properly configured \n",
    "machine. You are **not** expected to fill anything in this part.\n",
    "\n",
    "- [Part 1](#part-1-exercise): Graded exercise\n",
    "\n",
    "This part contains the detailed questions and assignments of the graded exercise. You\n",
    "are expected to fill this part with your own code and your own answers to the questions.\n",
    "\n",
    "⚠️ The deadline to hand-in a PDF report along this complete notebook with \n",
    "[Part 1](#part-1-exercise) filled is **October 2nd, 2024 at 23h59**. No submissions \n",
    "will be accepted past this deadline. ⚠️\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc424a6e-3e0f-45b6-8e12-f9abf739debe",
   "metadata": {},
   "source": [
    "## Part 0: Demonstration\n",
    "\n",
    "### 0. Introduction\n",
    "\n",
    "The **goals** of this exercise are:\n",
    "\n",
    "* Extracting statistical features of acoustic signals to discriminate between normal and abnormal samples\n",
    "* Defining a threshold-based anomaly detection rule based on normal samples only\n",
    "\n",
    "The **requirements** are:\n",
    "* Your method can use a single feature or a combination of features\n",
    "* You must include visualization in a meaningful way\n",
    "* Your method must be developed based on the **TRAIN set** only\n",
    "\n",
    "This part contains an example on a toy dataset.\n",
    "\n",
    "1. **Train**: training set containing only healthy (normal) samples\n",
    "2. **Test**: test set containing both normal and abnormal samples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d57c54a-fee5-4f02-98e3-dc9eda99688d",
   "metadata": {},
   "source": [
    "### 1. Notebook Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4a457d7-efbb-44c5-bfd7-3f456f154cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Packages for visualization\n",
    "%pip install seaborn matplotlib-venn --upgrade --quiet\n",
    "# Packages for feature extraction and machine learning\n",
    "%pip install librosa numpy pandas scipy --upgrade --quiet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3ffca958-becb-48ed-9808-011ce7086841",
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib_venn import venn2, venn3\n",
    "import numpy as np\n",
    "from numpy.fft import fft\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import scipy.stats as scist\n",
    "\n",
    "# Matplotlib configuration for notebooks\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f5a3134-a41c-410b-b964-68c369f0f0f4",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 2. Load the Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "230ecb00-8c32-45ab-8197-bd30c3cc8556",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset from your disk (you must have downloaded the file)\n",
    "data = np.load(\"toy_data.npz\")\n",
    "\n",
    "# Extract the training and testing set\n",
    "train_set = data[\"train\"]\n",
    "test_set = data[\"test\"]\n",
    "\n",
    "train_set_size = train_set.shape[0]\n",
    "train_set_timesteps = train_set.shape[1]\n",
    "test_set_size = test_set.shape[0]\n",
    "test_set_timesteps = test_set.shape[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e477c50-f108-4219-a205-d1fbffe06f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# About the train dataset\n",
    "print(\n",
    "    f\"The TRAINING set contains {train_set_size} recordings. \"\n",
    "    f\"Each recording contains {train_set_timesteps} samples.\"\n",
    ")\n",
    "\n",
    "# About the test dataset\n",
    "print(\n",
    "    f\"The TESTING set contains {test_set_size} recordings. \"\n",
    "    f\"Each recording contains {test_set_timesteps} samples.\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f03bc4e-8b21-42e6-9b98-2aa7b2d640fa",
   "metadata": {},
   "source": [
    "### 3. Visual Signals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "707dcccf-8de5-4643-840b-0ff1a1376579",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 3, figsize=(16, 4))\n",
    "ax[0].plot(train_set[0])\n",
    "ax[0].set_title(\"TRAIN set Item 0\")\n",
    "ax[1].plot(test_set[0])\n",
    "ax[1].set_title(\"TEST set Item 0\")\n",
    "ax[2].plot(test_set[1])\n",
    "ax[2].set_title(\"TEST set Item 1\")\n",
    "\n",
    "for i in range(3):\n",
    "    ax[i].set_xlabel(\"Sample\")\n",
    "    ax[i].set_ylabel(\"Signal Intensity\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2bff888-7ec7-4447-8235-74b568b951bf",
   "metadata": {},
   "source": [
    "### 4. Compute Absolute Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0c322af6-56a4-483b-a2de-f93fb5832f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set_abs = np.abs(train_set)\n",
    "test_set_abs = np.abs(test_set)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f85c91dc-0a21-4f61-8525-d3cc41643803",
   "metadata": {},
   "source": [
    "### 5. Compute Min, Max, Centered Moments and Centralized Moments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e27236bf-4f8b-4dd3-b21a-ee7f638885ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute moments 1 to 4 on the training set and assign them in a Pandas dataframe\n",
    "train_moments = pd.DataFrame(\n",
    "    {\n",
    "        \"MaxAbs\": np.amax(train_set_abs, axis=1),\n",
    "        \"Mean\": np.mean(train_set, axis=1),\n",
    "        \"Variance\": scist.moment(train_set, moment=2, axis=1, nan_policy=\"propagate\"),\n",
    "        \"Skewness\": scist.skew(train_set, axis=1, bias=True, nan_policy=\"propagate\"),\n",
    "        \"Kurtosis\": scist.kurtosis(\n",
    "            train_set, axis=1, fisher=False, bias=True, nan_policy=\"propagate\"\n",
    "        ),\n",
    "        \"type\": \"train\",\n",
    "    }\n",
    ")\n",
    "\n",
    "# Compute moments 1 to 4 on the testing set and assign them in a Pandas dataframe\n",
    "test_moments = pd.DataFrame(\n",
    "    {\n",
    "        \"MaxAbs\": np.amax(test_set_abs, axis=1),\n",
    "        \"Mean\": np.mean(test_set, axis=1),\n",
    "        \"Variance\": scist.moment(test_set, moment=2, axis=1, nan_policy=\"propagate\"),\n",
    "        \"Skewness\": scist.skew(test_set, axis=1, bias=True, nan_policy=\"propagate\"),\n",
    "        \"Kurtosis\": scist.kurtosis(\n",
    "            test_set, axis=1, fisher=False, bias=True, nan_policy=\"propagate\"\n",
    "        ),\n",
    "        \"type\": \"test\",\n",
    "    }\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2319f48-5c33-4323-ae1d-508dd4543e2f",
   "metadata": {},
   "source": [
    "### 6. Fourier Transform and Spectrum Skewness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e2dace6a-1ce9-4d61-b858-93f3d08227a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove DC compoent and calculate FFT\n",
    "train_spectrum = 2.0 / train_set_size * np.abs(fft(train_set, axis=1, norm=None))\n",
    "test_spectrum = 2.0 / test_set_size * np.abs(fft(test_set, axis=1, norm=None))\n",
    "\n",
    "# Compute Skewness of the Sprectrum excluding the 0 [Hz] component\n",
    "train_moments[\"SpSkew\"] = scist.skew(\n",
    "    train_spectrum[:, 1:], axis=1, bias=True, nan_policy=\"propagate\"\n",
    ")\n",
    "test_moments[\"SpSkew\"] = scist.skew(\n",
    "    test_spectrum[:, 1:], axis=1, bias=True, nan_policy=\"propagate\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d9e7ec3-2580-4ec6-8c9b-c23830e52b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If working on macOS, you want to specify that the diagonal plot should be histograms to avoid errors when plotting\n",
    "# Otherwise you can leave the lines commented\n",
    "sns.pairplot(\n",
    "    train_moments,\n",
    "    vars=[c for c in train_moments.columns if c != \"type\"],\n",
    "    height=3,\n",
    "    plot_kws={\"alpha\": 0.4, \"s\": 40, \"edgecolor\": None},\n",
    ")\n",
    "#            diag_kind = 'hist',\n",
    "#           diag_kws = {'alpha': 0.4, 'log':True, 'density':True}\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "820bc176-aa46-451c-8939-07b0befeff38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If working on macOS, you want to specify that the diagonal plot should be histograms to avoid errors when plotting\n",
    "# Otherwise you can leave the lines commented\n",
    "sns.pairplot(\n",
    "    test_moments,\n",
    "\n",
    "    vars=[c for c in test_moments.columns if c != \"type\"],\n",
    "\n",
    "    height=3,\n",
    "\n",
    "    plot_kws={\"alpha\": 0.4, \"s\": 40, \"edgecolor\": None},\n",
    ")\n",
    "# ,\n",
    "#            diag_kind = 'hist',\n",
    "#           diag_kws = {'alpha': 0.4, 'log':True, 'density':True})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "616af22e-b3ac-45e4-9ac1-2f70424bd0a8",
   "metadata": {},
   "source": [
    "### 7. Defining thresholds based on normal data percentiles\n",
    "\n",
    "Here, we define an anomaly detection rule based on thresholds, corresponding to the [0.5, 99.5] percentiles of the feature distributions of normal samples. In other words, a feature higher than 99.5% or lower than 0.5% of the normal features is considered an anomaly. Using only normal samples to base the decision is motivated by the facts that:\n",
    "\n",
    "1. Few anomalies are observed\n",
    "2. We cannot observe every possible anomaly\n",
    "\n",
    "Anomalies are outliers with respect to the normal data distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "052912c1-4ba9-4f11-8589-52cc132e7b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "quantiles = [0.5, 99.5]\n",
    "limit_max_abs = np.percentile(train_moments.loc[:, \"MaxAbs\"], quantiles)\n",
    "limit_mean = np.percentile(train_moments.loc[:, \"Mean\"], quantiles)\n",
    "limit_variance = np.percentile(train_moments.loc[:, \"Variance\"], quantiles)\n",
    "lim_skew = np.percentile(train_moments.loc[:, \"Skewness\"], quantiles)\n",
    "limit_kurtosis = np.percentile(train_moments.loc[:, \"Kurtosis\"], quantiles)\n",
    "limit_sp_skew = np.percentile(train_moments.loc[:, \"SpSkew\"], quantiles)\n",
    "\n",
    "train_percentage = np.linspace(0, 100, train_set_size)  # From 0% to 100%\n",
    "\n",
    "plt.figure(1, figsize=(15, 8))\n",
    "plt.clf()\n",
    "plt.suptitle(\n",
    "    \"Fig.1: Percentiles of the 4 first centered Moment for the training set.\",\n",
    "    fontsize=20,\n",
    ")\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.plot(\n",
    "    np.sort(train_moments.loc[:, \"Mean\"]), train_percentage, marker=\".\", linewidth=0\n",
    ")\n",
    "plt.plot([limit_mean[0], limit_mean[0]], plt.ylim(), color=\"k\")\n",
    "plt.plot([limit_mean[1], limit_mean[1]], plt.ylim(), color=\"k\")\n",
    "plt.xlabel(\"Value of the Mean-percentile\")\n",
    "plt.ylabel(\"Percentile\")\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.plot(\n",
    "    np.sort(train_moments.loc[:, \"Variance\"]), train_percentage, marker=\".\", linewidth=0\n",
    ")\n",
    "plt.plot([limit_variance[0], limit_variance[0]], plt.ylim(), color=\"k\")\n",
    "plt.plot([limit_variance[1], limit_variance[1]], plt.ylim(), color=\"k\")\n",
    "plt.xlabel(\"Value of the Variance-percentile\")\n",
    "plt.ylabel(\"Percentile\")\n",
    "plt.subplot(2, 2, 3)\n",
    "plt.plot(\n",
    "    np.sort(train_moments.loc[:, \"Skewness\"]), train_percentage, marker=\".\", linewidth=0\n",
    ")\n",
    "plt.plot([lim_skew[0], lim_skew[0]], plt.ylim(), color=\"k\")\n",
    "plt.plot([lim_skew[1], lim_skew[1]], plt.ylim(), color=\"k\")\n",
    "plt.xlabel(\"Value of the Skewness-percentile\")\n",
    "plt.ylabel(\"Percentile\")\n",
    "plt.subplot(2, 2, 4)\n",
    "plt.plot(\n",
    "    np.sort(train_moments.loc[:, \"Kurtosis\"]), train_percentage, marker=\".\", linewidth=0\n",
    ")\n",
    "plt.plot([limit_kurtosis[0], limit_kurtosis[0]], plt.ylim(), color=\"k\")\n",
    "plt.plot([limit_kurtosis[1], limit_kurtosis[1]], plt.ylim(), color=\"k\")\n",
    "plt.xlabel(\"Value of the Kurtosis-percentile\")\n",
    "plt.ylabel(\"Percentile\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32b2bafa-71b3-410d-8270-04969fdf5707",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(1, figsize=(15, 8))\n",
    "plt.clf()\n",
    "plt.suptitle(\"Fig.2: Percentiles of MaxAbs and SpSkew\", fontsize=20)\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(np.sort(train_moments.loc[:, \"MaxAbs\"]), train_percentage, marker=\".\", linewidth=0)\n",
    "plt.plot([limit_max_abs[0], limit_max_abs[0]], plt.ylim(), color=\"k\")\n",
    "plt.plot([limit_max_abs[1], limit_max_abs[1]], plt.ylim(), color=\"k\")\n",
    "plt.xlabel(\"Value of the MaxAbs-percentile\")\n",
    "plt.ylabel(\"Percentile\")\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(np.sort(train_moments.loc[:, \"SpSkew\"]), train_percentage, marker=\".\", linewidth=0)\n",
    "plt.plot([limit_sp_skew[0], limit_sp_skew[0]], plt.ylim(), color=\"k\")\n",
    "plt.plot([limit_sp_skew[1], limit_sp_skew[1]], plt.ylim(), color=\"k\")\n",
    "plt.xlabel(\"Value of the SpSkew-percentile\")\n",
    "plt.ylabel(\"Percentile\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dff1dc9b-49c5-4a5e-bcfd-bfcbc24d7d8c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "quantiles = [0.2, 99.8]\n",
    "limit_max_abs = np.percentile(train_moments.loc[:, \"MaxAbs\"], quantiles)\n",
    "limit_mean = np.percentile(train_moments.loc[:, \"Mean\"], quantiles)\n",
    "limit_variance = np.percentile(train_moments.loc[:, \"Variance\"], quantiles)\n",
    "lim_skew = np.percentile(train_moments.loc[:, \"Skewness\"], quantiles)\n",
    "limit_kurtosis = np.percentile(train_moments.loc[:, \"Kurtosis\"], quantiles)\n",
    "limit_sp_skew = np.percentile(train_moments.loc[:, \"SpSkew\"], quantiles)\n",
    "\n",
    "test_percentage = np.linspace(0, 100, test_set_size)  # from 0% to 100% with step 100/nTrain\n",
    "\n",
    "plt.figure(2, figsize=(15, 10))\n",
    "plt.clf()\n",
    "plt.suptitle(\n",
    "    \"Fig.3: Percentiles of the 4 first centered moments \"\n",
    "    + \"for both training and test sets with proposed thresholds.\",\n",
    "    fontsize=20,\n",
    ")\n",
    "plt.subplot(3, 2, 1)\n",
    "plt.plot(np.sort(train_moments.loc[:, \"Mean\"]), train_percentage, marker=\".\", linewidth=0)\n",
    "plt.plot(np.sort(test_moments.loc[:, \"Mean\"]), test_percentage, marker=\".\", linewidth=0)\n",
    "plt.plot([limit_mean[0], limit_mean[0]], plt.ylim(), color=\"k\")\n",
    "plt.plot([limit_mean[1], limit_mean[1]], plt.ylim(), color=\"k\")\n",
    "plt.xlabel(\"Mean\")\n",
    "plt.ylabel(\"Percentile\")\n",
    "\n",
    "plt.subplot(3, 2, 2)\n",
    "plt.plot(np.sort(train_moments.loc[:, \"Variance\"]), train_percentage, marker=\".\", linewidth=0)\n",
    "plt.plot(np.sort(test_moments.loc[:, \"Variance\"]), test_percentage, marker=\".\", linewidth=0)\n",
    "plt.plot([limit_variance[0], limit_variance[0]], plt.ylim(), color=\"k\")\n",
    "plt.plot([limit_variance[1], limit_variance[1]], plt.ylim(), color=\"k\")\n",
    "plt.xlabel(\"Variance\")\n",
    "plt.ylabel(\"Percentile\")\n",
    "\n",
    "plt.subplot(3, 2, 3)\n",
    "plt.plot(np.sort(train_moments.loc[:, \"Skewness\"]), train_percentage, marker=\".\", linewidth=0)\n",
    "plt.plot(np.sort(test_moments.loc[:, \"Skewness\"]), test_percentage, marker=\".\", linewidth=0)\n",
    "plt.plot([lim_skew[0], lim_skew[0]], plt.ylim(), color=\"k\")\n",
    "plt.plot([lim_skew[1], lim_skew[1]], plt.ylim(), color=\"k\")\n",
    "plt.xlabel(\"Skewness\")\n",
    "plt.ylabel(\"Percentile\")\n",
    "# plt.xlim([-50,50])\n",
    "\n",
    "plt.subplot(3, 2, 4)\n",
    "plt.plot(np.sort(train_moments.loc[:, \"Kurtosis\"]), train_percentage, marker=\".\", linewidth=0)\n",
    "plt.plot(np.sort(test_moments.loc[:, \"Kurtosis\"]), test_percentage, marker=\".\", linewidth=0)\n",
    "plt.plot([limit_kurtosis[0], limit_kurtosis[0]], plt.ylim(), color=\"k\")\n",
    "plt.plot([limit_kurtosis[1], limit_kurtosis[1]], plt.ylim(), color=\"k\")\n",
    "plt.xlabel(\"Kurtosis\")\n",
    "plt.ylabel(\"Percentile\")\n",
    "# plt.xlim([-50,5000])\n",
    "plt.figlegend(\n",
    "    [\"Train\", \"Test\", \"Threshold\"], loc=\"lower center\", ncol=5, labelspacing=0.0\n",
    ")\n",
    "\n",
    "plt.subplot(3, 2, 5)\n",
    "plt.plot(np.sort(train_moments.loc[:, \"MaxAbs\"]), train_percentage, marker=\".\", linewidth=0)\n",
    "plt.plot(np.sort(test_moments.loc[:, \"MaxAbs\"]), test_percentage, marker=\".\", linewidth=0)\n",
    "plt.plot([limit_max_abs[0], limit_max_abs[0]], plt.ylim(), color=\"k\")\n",
    "plt.plot([limit_max_abs[1], limit_max_abs[1]], plt.ylim(), color=\"k\")\n",
    "plt.xlabel(\"MaxAbs\")\n",
    "plt.ylabel(\"Percentile\")\n",
    "\n",
    "plt.subplot(3, 2, 6)\n",
    "plt.plot(np.sort(train_moments.loc[:, \"SpSkew\"]), train_percentage, marker=\".\", linewidth=0)\n",
    "plt.plot(np.sort(test_moments.loc[:, \"SpSkew\"]), test_percentage, marker=\".\", linewidth=0)\n",
    "plt.plot([limit_sp_skew[0], limit_sp_skew[0]], plt.ylim(), color=\"k\")\n",
    "plt.plot([limit_sp_skew[1], limit_sp_skew[1]], plt.ylim(), color=\"k\")\n",
    "plt.xlabel(\"SpSkew\")\n",
    "plt.ylabel(\"Percentile\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fec6450-d48a-4d01-88b2-0dc51e9e4a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select points either below 0.5 percentile or above 99.5 percentile of each moments and min-max limits\n",
    "abn_train_mean = np.where(\n",
    "    (train_moments.loc[:, \"Mean\"] < limit_mean[0])\n",
    "    | (train_moments.loc[:, \"Mean\"] > limit_mean[1])\n",
    ")[0]\n",
    "abn_train_variance = np.where(\n",
    "    (train_moments.loc[:, \"Variance\"] < limit_variance[0])\n",
    "    | (train_moments.loc[:, \"Variance\"] > limit_variance[1])\n",
    ")[0]\n",
    "abn_train_skew = np.where(\n",
    "    (train_moments.loc[:, \"Skewness\"] < lim_skew[0])\n",
    "    | (train_moments.loc[:, \"Skewness\"] > lim_skew[1])\n",
    ")[0]\n",
    "abn_train_kurtosis = np.where(\n",
    "    (train_moments.loc[:, \"Kurtosis\"] < limit_kurtosis[0])\n",
    "    | (train_moments.loc[:, \"Kurtosis\"] > limit_kurtosis[1])\n",
    ")[0]\n",
    "\n",
    "abn_train_max_abs = np.where(\n",
    "    (train_moments.loc[:, \"MaxAbs\"] < limit_max_abs[0])\n",
    "    | (train_moments.loc[:, \"MaxAbs\"] > limit_max_abs[1])\n",
    ")[0]\n",
    "abn_train_sp_skew = np.where(\n",
    "    (train_moments.loc[:, \"SpSkew\"] < limit_sp_skew[0])\n",
    "    | (train_moments.loc[:, \"SpSkew\"] > limit_sp_skew[1])\n",
    ")[0]\n",
    "\n",
    "all_abnormalities_train = np.unique(\n",
    "    np.concatenate(\n",
    "        (\n",
    "            abn_train_mean,\n",
    "            abn_train_variance,\n",
    "            abn_train_skew,\n",
    "            abn_train_kurtosis,\n",
    "            abn_train_max_abs,\n",
    "            abn_train_sp_skew,\n",
    "        )\n",
    "    )\n",
    ")\n",
    "\n",
    "# Compare sets together. Loop over each pair of sets and compute how many IDs they have in common\n",
    "train_dict = {\n",
    "    \"Mean\": abn_train_mean,\n",
    "    \"Variance\": abn_train_variance,\n",
    "    \"Skewness\": abn_train_skew,\n",
    "    \"Kurtsosis\": abn_train_kurtosis,\n",
    "    \"MaxAbs\": abn_train_max_abs,\n",
    "    \"SpSkew\": abn_train_sp_skew,\n",
    "}\n",
    "train_dict_keys = list(train_dict.keys())\n",
    "\n",
    "print(\"TRAINING DATASET\")\n",
    "for i in range(train_dict.__len__()):\n",
    "    k1 = train_dict_keys[i]\n",
    "    print(\"{}: {} anomalies found.\".format(k1, train_dict[k1].__len__()))\n",
    "\n",
    "anomalies_count_train = np.array(\n",
    "    [np.sum([i in train_dict[k] for k in train_dict]) for i in range(train_set_size)]\n",
    ")\n",
    "for i in range(1, 5):\n",
    "    print(\n",
    "        \"{} samples are found as anomalous by at least {} selection methods.\".format(\n",
    "            len(np.where(anomalies_count_train >= i)[0]), i\n",
    "        )\n",
    "    )\n",
    "\n",
    "######################################################################################################\n",
    "\n",
    "# select points either below 0.2 percentile or above 99.8 percentile of each moments and min-max limits\n",
    "abn_test_mean = np.where(\n",
    "    (test_moments.loc[:, \"Mean\"] < limit_mean[0])\n",
    "    | (test_moments.loc[:, \"Mean\"] > limit_mean[1])\n",
    ")[0]\n",
    "abn_test_variance = np.where(\n",
    "    (test_moments.loc[:, \"Variance\"] < limit_variance[0])\n",
    "    | (test_moments.loc[:, \"Variance\"] > limit_variance[1])\n",
    ")[0]\n",
    "abn_test_skew = np.where(\n",
    "    (test_moments.loc[:, \"Skewness\"] < lim_skew[0])\n",
    "    | (test_moments.loc[:, \"Skewness\"] > lim_skew[1])\n",
    ")[0]\n",
    "abn_test_kurtosis = np.where(\n",
    "    (test_moments.loc[:, \"Kurtosis\"] < limit_kurtosis[0])\n",
    "    | (test_moments.loc[:, \"Kurtosis\"] > limit_kurtosis[1])\n",
    ")[0]\n",
    "\n",
    "abn_test_max_abs = np.where(\n",
    "    (test_moments.loc[:, \"MaxAbs\"] < limit_max_abs[0])\n",
    "    | (test_moments.loc[:, \"MaxAbs\"] > limit_max_abs[1])\n",
    ")[0]\n",
    "abn_test_sp_skew = np.where(\n",
    "    (test_moments.loc[:, \"SpSkew\"] < limit_sp_skew[0])\n",
    "    | (test_moments.loc[:, \"SpSkew\"] > limit_sp_skew[1])\n",
    ")[0]\n",
    "\n",
    "allAbnormalities = np.unique(\n",
    "    np.concatenate(\n",
    "        (\n",
    "            abn_test_mean,\n",
    "            abn_test_variance,\n",
    "            abn_test_skew,\n",
    "            abn_test_kurtosis,\n",
    "            abn_test_max_abs,\n",
    "            abn_test_sp_skew,\n",
    "        )\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "# compare sets together. Here I loop over each pair of set and compute how many ID they have in common\n",
    "test_dict = {\n",
    "    \"Mean\": abn_test_mean,\n",
    "    \"Variance\": abn_test_variance,\n",
    "    \"Skewness\": abn_test_skew,\n",
    "    \"Kurtosis\": abn_test_kurtosis,\n",
    "    \"MaxAbs\": abn_test_max_abs,\n",
    "    \"SpSkew\": abn_test_sp_skew,\n",
    "}\n",
    "keysTest = list(test_dict.keys())\n",
    "\n",
    "\n",
    "print(\"\\nTEST DATASET\")\n",
    "for ii in range(test_dict.__len__()):\n",
    "    k1 = keysTest[ii]\n",
    "    print(f\"{k1}: {len(test_dict[k1])} anomalies found.\")\n",
    "\n",
    "countAbnormalTest = np.array(\n",
    "    [np.sum([i in test_dict[k] for k in test_dict]) for i in range(test_set_size)]\n",
    ")\n",
    "for i in range(1, 5):\n",
    "    print(\n",
    "        f\"{len(np.where(countAbnormalTest >= i)[0])} samples are found as anomalous \"\n",
    "        f\"by at least {i} selection methods.\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5098d35-3b3c-438e-b35c-58a837f7fc20",
   "metadata": {},
   "outputs": [],
   "source": [
    "fft = np.abs(np.fft.fft(train_set[1]))[: train_set_timesteps // 2]\n",
    "plt.plot(fft, label=\"Normal test sample\")\n",
    "fft = np.abs(np.fft.fft(test_set[0]))[: test_set_timesteps // 2]\n",
    "plt.plot(fft, label=\"Abnormal test sample\")\n",
    "plt.legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e5ade3f-7af0-42f2-996d-540041c61659",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(3, figsize=(15, 8))\n",
    "plt.clf()\n",
    "plt.suptitle(\n",
    "    \"Fig.4:Venn diagram between recordings considered abnormals by features 'Mean' and 'Variance'.\",\n",
    "    fontsize=20,\n",
    ")\n",
    "\n",
    "loc_train_mean = (train_moments.loc[:, \"Mean\"] < limit_mean[0]) | (\n",
    "    train_moments.loc[:, \"Mean\"] > limit_mean[1]\n",
    ")\n",
    "loc_train_variance = (train_moments.loc[:, \"Variance\"] < limit_variance[0]) | (\n",
    "    train_moments.loc[:, \"Variance\"] > limit_variance[1]\n",
    ")\n",
    "a1 = np.sum(loc_train_mean & loc_train_variance)\n",
    "a2 = np.sum(loc_train_mean & ~loc_train_variance)\n",
    "a3 = np.sum(~loc_train_mean & loc_train_variance)\n",
    "\n",
    "plt.subplot(121)\n",
    "venn2(subsets=(a2, a3, a1), set_labels=(\"Mean\", \"Variance\"))\n",
    "plt.title(\"Training dataset\")\n",
    "\n",
    "\n",
    "loc_test_mean = (test_moments.loc[:, \"Mean\"] < limit_mean[0]) | (\n",
    "    test_moments.loc[:, \"Mean\"] > limit_mean[1]\n",
    ")\n",
    "loc_test_variance = (test_moments.loc[:, \"Variance\"] < limit_variance[0]) | (\n",
    "    test_moments.loc[:, \"Variance\"] > limit_variance[1]\n",
    ")\n",
    "a1 = np.sum(loc_test_mean & loc_test_variance)\n",
    "a2 = np.sum(loc_test_mean & ~loc_test_variance)\n",
    "a3 = np.sum(~loc_test_mean & loc_test_variance)\n",
    "\n",
    "\n",
    "plt.subplot(122)\n",
    "venn2(subsets=(a2, a3, a1), set_labels=(\"Mean\", \"Variance\"))\n",
    "plt.title(\"Test dataset\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b0742f-cf56-45ac-9cbc-59f19877a8cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(3, figsize=(15, 8))\n",
    "plt.clf()\n",
    "plt.suptitle(\n",
    "    \"Fig.5:Venn diagram between recordings considered abnormals by features 'Kurtosis' and 'MaxAbs'.\",\n",
    "    fontsize=20,\n",
    ")\n",
    "\n",
    "loc_train_kurtosis = (train_moments.loc[:, \"Kurtosis\"] < limit_kurtosis[0]) | (\n",
    "    train_moments.loc[:, \"Kurtosis\"] > limit_kurtosis[1]\n",
    ")\n",
    "loc_train_max_abs = (train_moments.loc[:, \"MaxAbs\"] < limit_max_abs[0]) | (\n",
    "    train_moments.loc[:, \"MaxAbs\"] > limit_max_abs[1]\n",
    ")\n",
    "a1 = np.sum(loc_train_kurtosis & loc_train_max_abs)\n",
    "a2 = np.sum(loc_train_kurtosis & ~loc_train_max_abs)\n",
    "a3 = np.sum(~loc_train_kurtosis & loc_train_max_abs)\n",
    "\n",
    "plt.subplot(121)\n",
    "venn2(subsets=(a2, a3, a1), set_labels=(\"Kurtosis\", \"MaxAbs\"))\n",
    "plt.title(\"Training dataset\")\n",
    "\n",
    "\n",
    "loc_test_kurtosis = (test_moments.loc[:, \"Kurtosis\"] < limit_kurtosis[0]) | (\n",
    "    test_moments.loc[:, \"Kurtosis\"] > limit_kurtosis[1]\n",
    ")\n",
    "loc_test_max_abs = (test_moments.loc[:, \"MaxAbs\"] < limit_max_abs[0]) | (\n",
    "    test_moments.loc[:, \"MaxAbs\"] > limit_max_abs[1]\n",
    ")\n",
    "a1 = np.sum(loc_test_kurtosis & loc_test_max_abs)\n",
    "a2 = np.sum(loc_test_kurtosis & ~loc_test_max_abs)\n",
    "a3 = np.sum(~loc_test_kurtosis & loc_test_max_abs)\n",
    "\n",
    "\n",
    "plt.subplot(122)\n",
    "venn2(subsets=(a2, a3, a1), set_labels=(\"Kurtosis\", \"MaxAbs\"))\n",
    "plt.title(\"Test dataset\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c00616f2-f33a-495a-baeb-d48526fe8e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(3, figsize=(15, 8))\n",
    "plt.clf()\n",
    "plt.suptitle(\n",
    "    \"Fig.6:Venn diagram between recordings considered abnormals by features 'Kurtosis' and 'SpSkew'.\",\n",
    "    fontsize=20,\n",
    ")\n",
    "\n",
    "loc_train_kurtosis = (train_moments.loc[:, \"Kurtosis\"] < limit_kurtosis[0]) | (\n",
    "    train_moments.loc[:, \"Kurtosis\"] > limit_kurtosis[1]\n",
    ")\n",
    "loc_train_sp_skew = (train_moments.loc[:, \"SpSkew\"] < limit_sp_skew[0]) | (\n",
    "    train_moments.loc[:, \"SpSkew\"] > limit_sp_skew[1]\n",
    ")\n",
    "a1 = np.sum(loc_train_kurtosis & loc_train_sp_skew)\n",
    "a2 = np.sum(loc_train_kurtosis & ~loc_train_sp_skew)\n",
    "a3 = np.sum(~loc_train_kurtosis & loc_train_sp_skew)\n",
    "\n",
    "plt.subplot(121)\n",
    "venn2(subsets=(a2, a3, a1), set_labels=(\"Kurtosis\", \"SpSkew\"))\n",
    "plt.title(\"Training dataset\")\n",
    "\n",
    "\n",
    "loc_test_kurtosis = (test_moments.loc[:, \"Kurtosis\"] < limit_kurtosis[0]) | (\n",
    "    test_moments.loc[:, \"Kurtosis\"] > limit_kurtosis[1]\n",
    ")\n",
    "loc_test_sp_skew = (test_moments.loc[:, \"SpSkew\"] < limit_sp_skew[0]) | (\n",
    "    test_moments.loc[:, \"SpSkew\"] > limit_sp_skew[1]\n",
    ")\n",
    "a1 = np.sum(loc_test_kurtosis & loc_test_sp_skew)\n",
    "a2 = np.sum(loc_test_kurtosis & ~loc_test_sp_skew)\n",
    "a3 = np.sum(~loc_test_kurtosis & loc_test_sp_skew)\n",
    "\n",
    "\n",
    "plt.subplot(122)\n",
    "venn2(subsets=(a2, a3, a1), set_labels=(\"Kurtosis\", \"SpSkew\"))\n",
    "plt.title(\"Test dataset\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe1578a7-f2b8-4d9a-b105-11849837cb7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "loc_test_kurtosis = (test_moments.loc[:, \"Kurtosis\"] < limit_kurtosis[0]) | (\n",
    "    test_moments.loc[:, \"Kurtosis\"] > limit_kurtosis[1]\n",
    ")\n",
    "loc_test_mean = (test_moments.loc[:, \"Mean\"] < limit_mean[0]) | (\n",
    "    test_moments.loc[:, \"Mean\"] > limit_mean[1]\n",
    ")\n",
    "loc_test_sp_skew = (test_moments.loc[:, \"SpSkew\"] < limit_sp_skew[0]) | (\n",
    "    test_moments.loc[:, \"SpSkew\"] > limit_sp_skew[1]\n",
    ")\n",
    "\n",
    "a1 = np.sum((loc_test_sp_skew & ~loc_test_kurtosis) & ~loc_test_mean)\n",
    "a2 = np.sum((~loc_test_sp_skew & loc_test_kurtosis) & ~loc_test_mean)\n",
    "a3 = np.sum((loc_test_sp_skew & loc_test_kurtosis) & ~loc_test_mean)\n",
    "a4 = np.sum((~loc_test_sp_skew & ~loc_test_kurtosis) & loc_test_mean)\n",
    "a5 = np.sum((loc_test_sp_skew & ~loc_test_kurtosis) & loc_test_mean)\n",
    "a6 = np.sum((~loc_test_sp_skew & loc_test_kurtosis) & loc_test_mean)\n",
    "a7 = np.sum((loc_test_sp_skew & loc_test_kurtosis) & loc_test_mean)\n",
    "\n",
    "plt.figure(4, figsize=(15, 8))\n",
    "plt.clf()\n",
    "plt.suptitle(\n",
    "    \"Fig.7:Venn diagram between three features (for the Test dataset).\", fontsize=20\n",
    ")\n",
    "\n",
    "venn3(subsets=(a1, a2, a3, a4, a5, a6, a7), set_labels=(\"SpSkew\", \"Kurtosis\", \"Mean\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cb9ade7-6caf-4276-bb61-ac58e8692565",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9986618-73e8-4f44-b002-04973f4b1fa9",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Part 1: Exercise\n",
    "\n",
    "⚠️ **This part contains the actual graded exercise 1 questions.** ⚠️\n",
    "\n",
    "Find your own features for a real acoustic dataset from industrial machines!\n",
    "\n",
    "Acoustic emission and vibration monitoring play integral roles in structural health \n",
    "monitoring (SHM) by providing crucial insights into the condition and integrity of \n",
    "structures. Acoustic Emission involves the detection of transient stress waves or \n",
    "acoustic signals emitted by materials when they undergo deformation or damage. Vibration \n",
    "monitoring with accelerometers is widely used to assess the structural integrity of \n",
    "various systems, from bridges and buildings to rotating machinery. Vibration analysis \n",
    "can reveal changes in natural frequencies, mode shapes, and damping characteristics, \n",
    "which can be indicative of structural damage or degradation. \n",
    "\n",
    "### Dataset Description\n",
    "\n",
    "For this exercise, we consider acoustic emissions from two machines: a **pump** and a \n",
    "**valve**. The datasets come as follows:\n",
    "\n",
    "* **pump**\n",
    "    * train set containing data from pump in good working conditions\n",
    "    * test set containing both healthy data and data from pumps with abnormal behaviors\n",
    "* **valve**\n",
    "    * train set containing data from valve in good working conditions\n",
    "    * test set containing both healthy data and data from valves with abnormal behaviors\n",
    "\n",
    "### Problem Description\n",
    "\n",
    "Using features introduced in [Part 0](#part-0-demonstration), other features presented \n",
    "during lectures and previous exercises, or any other features you deem useful, your goal\n",
    "is to identify features that can uncover anomalies present in the test dataset but not \n",
    "present in the training dataset. Selected features will be different for the pump and \n",
    "valve.\n",
    "\n",
    "### Questions\n",
    "\n",
    "**Question 1:** Generate plots of raw signals, FFT spectrums or spectrogram from the \n",
    "healthy data of both the pump and valve acoustic signals. Discuss the distinctions \n",
    "between the signals emitted by these two machines.\n",
    "\n",
    "Questions 2, 3 and 4 have to be answered separately for both the pump and valve datasets.\n",
    "\n",
    "**Question 2:** Visualize the raw signals, spectrum, and spectrograms of the test \n",
    "dataset for the pump/valve dataset. Are there any signals that appear abnormal to you?\n",
    "\n",
    "**Question 3:** Compute basic statistical features (mean, variance, skewness, and \n",
    "kurtosis) for both the training and test datasets of the pump and valve. Are there any \n",
    "abnormal signals that you can detect?\n",
    "\n",
    "**Question 4:** Find by yourself a feature or a combination of feature that help to \n",
    "uncover signals with abnormal behavior. Analyze whether the selected features trigger \n",
    "alarms for similar behavior or if some are specific to particular anomalies.\n",
    "\n",
    "**Question 5:** What are some potential limitations of the method suggested in this \n",
    "exercise for anomaly detection? Answer with at least 3 limitations.\n",
    "\n",
    "**Question 6:** Now that you have developed a set of features and thresholds for a \n",
    "valve/pump, imagine applying them to a different valve/pump. Would the discriminative \n",
    "power change? If so, how do you propose to mitigate it? Justify and provide concrete \n",
    "details.\n",
    "\n",
    "**Question 7:** Now imagine you're in a scenario where the are no anomalous samples \n",
    "available. How would you tackle this problem? Answer with an overview of your proposed \n",
    "approach and then provide concrete details regarding the method.\n",
    "\n",
    "\n",
    "The answers to those questions are expected in a **PDF report**. The full Jupyter notebook \n",
    "must also be submitted. A PDF report without any description, analysis, and discussion \n",
    "around the plots will not be considered valid.\n",
    "\n",
    "### Critical Points & Requirements\n",
    "\n",
    "Here is a unorded non-exhaustive list of some critical points which will be taken into \n",
    "account for grading:\n",
    "- Quality of the plots (legend, axes labels, plot titles, units, ...)\n",
    "- Discriminative power of the selected features (performance) on the `valve` dataset\n",
    "- Discriminative power of the selected features (performance) on the `pump` dataset\n",
    "- Quality of the scientific reasoning\n",
    "- Structure, clarity, and conciseness of the report\n",
    "- Quality of the code (useful but concise documentation, clarity of the code, ...)\n",
    "- ...\n",
    "\n",
    "⚠️ Your submitted notebook should be able to run on a properly configured environment\n",
    "(*c.f.* week 1 exercises). If you require additional packages, make sure to add a \n",
    "`pip install` command in a code cell (example in the following cell to install `gdown`.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07ef3974",
   "metadata": {},
   "source": [
    "### 0. Download the Dataset\n",
    "\n",
    "``gdown`` is a Python library and command-line tool that simplifies the process of downloading files from Google Drive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d913db99",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%pip install gdown --upgrade --quiet\n",
    "import gdown\n",
    "from scipy.signal import find_peaks\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aea1d738",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pump dataset\n",
    "gdown.download(id=\"1Vz1fhpu5xKJ4RI5maJh_3OweX9BpjBaM\", output=\"./pump.zip\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a096094-91d9-48a7-994d-498050703ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Valve dataset\n",
    "gdown.download(id=\"13bzdjL0gEc9hsGHjr0Qo8OMcMF5CrYbS\", output=\"./valve.zip\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09be0673",
   "metadata": {},
   "source": [
    "### 1. Extract the Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d21dd228",
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import os\n",
    "\n",
    "# List all ZIP files in the current directory\n",
    "zip_files = [file for file in os.listdir() if file.endswith(\".zip\")]\n",
    "\n",
    "# Loop through each ZIP file and unzip it in the same directory\n",
    "for zip_file in zip_files:\n",
    "    with zipfile.ZipFile(zip_file, \"r\") as zip_ref:\n",
    "        zip_ref.extractall()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bff5ae7d",
   "metadata": {},
   "source": [
    "### 2. Load the Audio Data\n",
    "\n",
    "In this code cell, we are using the `librosa` library to load audio files from two \n",
    "directories: `/valve` and `/pump`. The goal is to prepare audio data for further \n",
    "analysis or processing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "22dce0eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_wav_files(directory: str) -> np.ndarray:\n",
    "    \"\"\"Load all .WAV audio files in a directory as a 2D Numpy array\n",
    "\n",
    "    Args:\n",
    "        directory (str): Path to the directory containing the audio files\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Loaded audio files as a 2D array\n",
    "    \"\"\"\n",
    "    audio_data: list = []\n",
    "\n",
    "    # Iterate through files in the directory\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith(\".wav\") or filename.endswith(\".WAV\"):\n",
    "            file_path = os.path.join(directory, filename)\n",
    "\n",
    "            # Load audio file and append it to the list\n",
    "            audio, sr = librosa.load(file_path, sr=None)\n",
    "            audio_data.append(audio)\n",
    "\n",
    "    return np.array(audio_data)\n",
    "\n",
    "\n",
    "# Paths to the \"valve\" and \"pump\" directories\n",
    "valve_directory = \"./valve\"\n",
    "pump_directory = \"./pump\"\n",
    "\n",
    "# Load audio data for \"valve\" and \"pump\"\n",
    "valve_train_data = load_wav_files(os.path.join(valve_directory, \"train\"))\n",
    "valve_test_data = load_wav_files(os.path.join(valve_directory, \"test\"))\n",
    "pump_train_data = load_wav_files(os.path.join(pump_directory, \"train\"))\n",
    "pump_test_data = load_wav_files(os.path.join(pump_directory, \"test\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e68588",
   "metadata": {},
   "outputs": [],
   "source": [
    "valve_train_data_size = valve_train_data.shape\n",
    "valve_test_data_size = valve_test_data.shape\n",
    "pump_train_data_size = pump_train_data.shape\n",
    "pump_test_data_size = pump_test_data.shape\n",
    "\n",
    "print(f\"Valve Train Data Shape: {valve_train_data_size}\")\n",
    "print(f\"Valve Test Data Shape: {valve_test_data_size}\")\n",
    "print(f\"Pump Train Data Shape: {pump_train_data_size}\")\n",
    "print(f\"Pump Test Data Shape: {pump_test_data_size}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aa64f5e",
   "metadata": {},
   "source": [
    "⚠️ **You're expected to fill the notebook from here.** ⚠️\n",
    "\n",
    "### Question 1:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9a3122fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a subfolder in the current project directory\n",
    "output_dir = 'Figures'\n",
    "\n",
    "# Create the subfolder if it doesn't exist\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b00ceb6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_valve_samples(valve_data, num_samples=3,name = \"valve_or_pump\", sr=16000):\n",
    "    \"\"\"\n",
    "    Plots the time domain, frequency domain (FFT), and spectrograms for randomly selected samples from the valve dataset.\n",
    "\n",
    "    Parameters:\n",
    "    valve_data (numpy array): The dataset containing valve samples, each row is a separate sample.\n",
    "    num_samples (int): Number of samples to select and plot. Default is 3.\n",
    "    sr (int): The sample rate for the signals. Default is 16000 Hz.\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    # Randomly choose 'num_samples' rows from the dataset\n",
    "    random_indices = np.random.choice(valve_data.shape[0], num_samples, replace=False)\n",
    "\n",
    "    # Create subplots: 3 rows (Time Domain, FFT, Spectrogram) and `num_samples` columns\n",
    "    fig, axes = plt.subplots(3, num_samples, figsize=(18, 18))\n",
    "    fig.tight_layout(pad=4.0)\n",
    "\n",
    "    # Loop through each selected sample and plot\n",
    "    for i, idx in enumerate(random_indices):\n",
    "        # Get the sample data\n",
    "        one_valve_sample = valve_data[idx, :]\n",
    "\n",
    "        # Plot the time domain signal in the first row\n",
    "        axes[0, i].plot(np.linspace(0, len(one_valve_sample) / sr, len(one_valve_sample)), one_valve_sample, 'b')\n",
    "        axes[0, i].set_title(f'Time Domain - Sample {idx}')\n",
    "        axes[0, i].set_xlabel('Time (s)')\n",
    "        axes[0, i].set_ylabel('Amplitude')\n",
    "\n",
    "        # Perform FFT\n",
    "        X = np.fft.fft(one_valve_sample)\n",
    "        N = len(X)\n",
    "        n = np.arange(N)\n",
    "        T = N / sr\n",
    "        freq = n / T\n",
    "\n",
    "        # Plot the frequency domain (FFT) in the second row\n",
    "        axes[1, i].stem(freq, np.abs(X), 'b', markerfmt=\" \", basefmt=\"-b\")\n",
    "        axes[1, i].set_title(f'Freq Domain - Sample {idx}')\n",
    "        axes[1, i].set_xlabel('Freq (Hz)')\n",
    "        axes[1, i].set_ylabel('FFT Amplitude')\n",
    "        axes[1, i].set_xlim(0, sr/2)\n",
    "\n",
    "        # Perform Spectrogram\n",
    "        spec = librosa.stft(one_valve_sample, n_fft=1024, hop_length=512, window='hann')\n",
    "        spec_db = 20 * np.log10(np.abs(spec) + 1e-10)  # Convert to dB scale\n",
    "\n",
    "        # Plot the Spectrogram in the third row\n",
    "        vmin = -80\n",
    "        vmax = 0\n",
    "        img = librosa.display.specshow(spec_db, sr=sr, hop_length=512, x_axis='time', y_axis='linear', ax=axes[2, i], vmin=vmin, vmax=vmax)\n",
    "        fig.colorbar(img, ax=axes[2, i], format=\"%+2.0f dB\")  # Add colorbar to the spectrogram\n",
    "\n",
    "        axes[2, i].set_title(f'Spectrogram - Sample {idx}')\n",
    "        axes[2, i].set_xlabel('Time (s)')\n",
    "        axes[2, i].set_ylabel('Freq (Hz)')\n",
    "    if name != \"valve_or_pump\":\n",
    "\n",
    "        # Define the full file path for the image\n",
    "        file_path = os.path.join(output_dir, f'{name}_healthy_sample.png')\n",
    "\n",
    "        # Save the plot to the subfolder\n",
    "        plt.savefig(file_path, dpi=300)\n",
    "\n",
    "    # Show the plots\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51318b20",
   "metadata": {},
   "source": [
    "#### Train Valve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e496348c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plot_valve_samples(valve_train_data, num_samples=3, name=\"valve_train\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "334b5c9b",
   "metadata": {},
   "source": [
    "#### Train Pump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "501742f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_valve_samples(pump_train_data, num_samples=3, name=\"pump_train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "982682d4",
   "metadata": {},
   "source": [
    "### Question 2:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88c2c5a7",
   "metadata": {},
   "source": [
    "#### Test Valve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c439b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plot_valve_samples(valve_test_data, num_samples=5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8e3c31d",
   "metadata": {},
   "source": [
    "#### Test Pump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f6feb46",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_valve_samples(pump_test_data, num_samples=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5716a720",
   "metadata": {},
   "source": [
    "#### Ploting Specific cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2096b35f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Valve\n",
    "# Sample : 31, 98\n",
    "valve_indicies = [31, 98]\n",
    "\n",
    "# Pump\n",
    "# Sample : 103, 131, 151, 137\n",
    "pump_indicies = [103, 131, 151, 137]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d746efe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sr = 16000\n",
    "# Create subplots: 3 rows (Time Domain, FFT, Spectrogram) and `num_samples` columns\n",
    "fig, axes = plt.subplots(3, len(valve_indicies), figsize=(18, 18))\n",
    "fig.tight_layout(pad=4.0)\n",
    "\n",
    "# Loop through each selected sample and plot\n",
    "for i, idx in enumerate(valve_indicies):\n",
    "    # Get the sample data\n",
    "    one_valve_sample = valve_test_data[idx, :]\n",
    "\n",
    "    # Plot the time domain signal in the first row\n",
    "    axes[0, i].plot(np.linspace(0, len(one_valve_sample) / sr, len(one_valve_sample)), one_valve_sample, 'b')\n",
    "    axes[0, i].set_title(f'Time Domain - Sample {idx}')\n",
    "    axes[0, i].set_xlabel('Time (s)')\n",
    "    axes[0, i].set_ylabel('Amplitude')\n",
    "\n",
    "    # Perform FFT\n",
    "    X = np.fft.fft(one_valve_sample)\n",
    "    N = len(X)\n",
    "    n = np.arange(N)\n",
    "    T = N / sr\n",
    "    freq = n / T\n",
    "\n",
    "    # Plot the frequency domain (FFT) in the second row\n",
    "    axes[1, i].stem(freq, np.abs(X), 'b', markerfmt=\" \", basefmt=\"-b\")\n",
    "    axes[1, i].set_title(f'Freq Domain - Sample {idx}')\n",
    "    axes[1, i].set_xlabel('Freq (Hz)')\n",
    "    axes[1, i].set_ylabel('FFT Amplitude')\n",
    "    axes[1, i].set_xlim(0, sr/2)\n",
    "\n",
    "    # Perform Spectrogram\n",
    "    spec = librosa.stft(one_valve_sample, n_fft=1024, hop_length=512, window='hann')\n",
    "    spec_db = 20 * np.log10(np.abs(spec) + 1e-10)  # Convert to dB scale\n",
    "\n",
    "    # Plot the Spectrogram in the third row\n",
    "    vmin = -80\n",
    "    vmax = 0\n",
    "    img = librosa.display.specshow(spec_db, sr=sr, hop_length=512, x_axis='time', y_axis='linear', ax=axes[2, i], vmin=vmin, vmax=vmax)\n",
    "    fig.colorbar(img, ax=axes[2, i], format=\"%+2.0f dB\")  # Add colorbar to the spectrogram\n",
    "\n",
    "    axes[2, i].set_title(f'Spectrogram - Sample {idx}')\n",
    "    axes[2, i].set_xlabel('Time (s)')\n",
    "    axes[2, i].set_ylabel('Freq (Hz)')\n",
    "\n",
    "    # Show the plots\n",
    "\n",
    "# Define the full file path for the image\n",
    "file_path = os.path.join(output_dir, 'valve_anomalies_sample.png')\n",
    "\n",
    "# Save the plot to the subfolder\n",
    "plt.savefig(file_path, dpi=300)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38003435",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sr = 16000\n",
    "# Create subplots: 3 rows (Time Domain, FFT, Spectrogram) and `num_samples` columns\n",
    "fig, axes = plt.subplots(3, len(pump_indicies), figsize=(18, 18))\n",
    "fig.tight_layout(pad=4.0)\n",
    "\n",
    "# Loop through each selected sample and plot\n",
    "for i, idx in enumerate(pump_indicies):\n",
    "    # Get the sample data\n",
    "    one_valve_sample = pump_test_data[idx, :]\n",
    "\n",
    "    # Plot the time domain signal in the first row\n",
    "    axes[0, i].plot(np.linspace(0, len(one_valve_sample) / sr, len(one_valve_sample)), one_valve_sample, 'b')\n",
    "    axes[0, i].set_title(f'Time Domain - Sample {idx}')\n",
    "    axes[0, i].set_xlabel('Time (s)')\n",
    "    axes[0, i].set_ylabel('Amplitude')\n",
    "\n",
    "    # Perform FFT\n",
    "    X = np.fft.fft(one_valve_sample)\n",
    "    N = len(X)\n",
    "    n = np.arange(N)\n",
    "    T = N / sr\n",
    "    freq = n / T\n",
    "\n",
    "    # Plot the frequency domain (FFT) in the second row\n",
    "    axes[1, i].stem(freq, np.abs(X), 'b', markerfmt=\" \", basefmt=\"-b\")\n",
    "    axes[1, i].set_title(f'Freq Domain - Sample {idx}')\n",
    "    axes[1, i].set_xlabel('Freq (Hz)')\n",
    "    axes[1, i].set_ylabel('FFT Amplitude')\n",
    "    axes[1, i].set_xlim(0, sr/2)\n",
    "\n",
    "    # Perform Spectrogram\n",
    "    spec = librosa.stft(one_valve_sample, n_fft=1024, hop_length=512, window='hann')\n",
    "    spec_db = 20 * np.log10(np.abs(spec) + 1e-10)  # Convert to dB scale\n",
    "\n",
    "    # Plot the Spectrogram in the third row\n",
    "    vmin = -80\n",
    "    vmax = 0\n",
    "    img = librosa.display.specshow(spec_db, sr=sr, hop_length=512, x_axis='time', y_axis='linear', ax=axes[2, i], vmin=vmin, vmax=vmax)\n",
    "    fig.colorbar(img, ax=axes[2, i], format=\"%+2.0f dB\")  # Add colorbar to the spectrogram\n",
    "\n",
    "    axes[2, i].set_title(f'Spectrogram - Sample {idx}')\n",
    "    axes[2, i].set_xlabel('Time (s)')\n",
    "    axes[2, i].set_ylabel('Freq (Hz)')\n",
    "\n",
    "# Define the full file path for the image\n",
    "file_path = os.path.join(output_dir, 'pump_anomalies_sample.png')\n",
    "\n",
    "# Save the plot to the subfolder\n",
    "plt.savefig(file_path, dpi=300)\n",
    "plt.show()\n",
    "# Show the plots\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e8d2726",
   "metadata": {},
   "source": [
    "### Question 3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "41573645",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute moments 1 to 4 on the valve training set and assign them in a Pandas dataframe\n",
    "valve_train_moments = pd.DataFrame(\n",
    "    {\n",
    "        \"Mean\": np.mean(valve_train_data, axis=1),\n",
    "        \"Variance\": scist.moment(valve_train_data, moment=2, axis=1, nan_policy=\"propagate\"),\n",
    "        \"Skewness\": scist.skew(valve_train_data, axis=1, bias=True, nan_policy=\"propagate\"),\n",
    "        \"Kurtosis\": scist.kurtosis(\n",
    "            valve_train_data, axis=1, fisher=False, bias=True, nan_policy=\"propagate\"\n",
    "        ),\n",
    "        \"type\": \"train\",\n",
    "    }\n",
    ")\n",
    "\n",
    "# Compute moments 1 to 4 on the valve testing set and assign them in a Pandas dataframe\n",
    "valve_test_moments = pd.DataFrame(\n",
    "    {\n",
    "        \"Mean\": np.mean(valve_test_data, axis=1),\n",
    "        \"Variance\": scist.moment(valve_test_data, moment=2, axis=1, nan_policy=\"propagate\"),\n",
    "        \"Skewness\": scist.skew(valve_test_data, axis=1, bias=True, nan_policy=\"propagate\"),\n",
    "        \"Kurtosis\": scist.kurtosis(\n",
    "            valve_test_data, axis=1, fisher=False, bias=True, nan_policy=\"propagate\"\n",
    "        ),\n",
    "        \"type\": \"test\",\n",
    "    }\n",
    ")\n",
    "\n",
    "# Compute moments 1 to 4 on the pump training set and assign them in a Pandas dataframe\n",
    "pump_train_moments = pd.DataFrame(\n",
    "    {\n",
    "        \"Mean\": np.mean(pump_train_data, axis=1),\n",
    "        \"Variance\": scist.moment(pump_train_data, moment=2, axis=1, nan_policy=\"propagate\"),\n",
    "        \"Skewness\": scist.skew(pump_train_data, axis=1, bias=True, nan_policy=\"propagate\"),\n",
    "        \"Kurtosis\": scist.kurtosis(\n",
    "            pump_train_data, axis=1, fisher=False, bias=True, nan_policy=\"propagate\"\n",
    "        ),\n",
    "        \"type\": \"train\",\n",
    "    }\n",
    ")\n",
    "\n",
    "# Compute moments 1 to 4 on the pump testing set and assign them in a Pandas dataframe\n",
    "pump_test_moments = pd.DataFrame(\n",
    "    {\n",
    "        \"Mean\": np.mean(pump_test_data, axis=1),\n",
    "        \"Variance\": scist.moment(pump_test_data, moment=2, axis=1, nan_policy=\"propagate\"),\n",
    "        \"Skewness\": scist.skew(pump_test_data, axis=1, bias=True, nan_policy=\"propagate\"),\n",
    "        \"Kurtosis\": scist.kurtosis(\n",
    "            pump_test_data, axis=1, fisher=False, bias=True, nan_policy=\"propagate\"\n",
    "        ),\n",
    "        \"type\": \"test\"\n",
    "    }\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0e47351",
   "metadata": {},
   "source": [
    "#### Valve ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fb1852f",
   "metadata": {},
   "outputs": [],
   "source": [
    "quantiles = [0.5, 99.5]\n",
    "limit_mean = np.percentile(valve_train_moments.loc[:, \"Mean\"], quantiles)\n",
    "limit_variance = np.percentile(valve_train_moments.loc[:, \"Variance\"], quantiles)\n",
    "lim_skew = np.percentile(valve_train_moments.loc[:, \"Skewness\"], quantiles)\n",
    "limit_kurtosis = np.percentile(valve_train_moments.loc[:, \"Kurtosis\"], quantiles)\n",
    "valve_train_percentage = np.linspace(0, 100, valve_train_data_size[0])  # From 0% to 100%\n",
    "\n",
    "\n",
    "#Plotting the percentiles of mean, variance, skew and kurtosis for the valve training set\n",
    "plt.figure(1, figsize=(15, 8))\n",
    "plt.clf()\n",
    "plt.suptitle(\n",
    "    \"Fig.1: Percentiles of the 4 first centered Moment for the training set for the valve.\",\n",
    "    fontsize=20,\n",
    ")\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.plot(\n",
    "    np.sort(valve_train_moments.loc[:, \"Mean\"]), valve_train_percentage, marker=\".\", linewidth=0\n",
    ")\n",
    "plt.plot([limit_mean[0], limit_mean[0]], plt.ylim(), color=\"k\")\n",
    "plt.plot([limit_mean[1], limit_mean[1]], plt.ylim(), color=\"k\")\n",
    "plt.xlabel(\"Value of the Mean-percentile\")\n",
    "plt.ylabel(\"Percentile\")\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.plot(\n",
    "    np.sort(valve_train_moments.loc[:, \"Variance\"]), valve_train_percentage, marker=\".\", linewidth=0\n",
    ")\n",
    "plt.plot([limit_variance[0], limit_variance[0]], plt.ylim(), color=\"k\")\n",
    "plt.plot([limit_variance[1], limit_variance[1]], plt.ylim(), color=\"k\")\n",
    "plt.xlabel(\"Value of the Variance-percentile\")\n",
    "plt.ylabel(\"Percentile\")\n",
    "plt.subplot(2, 2, 3)\n",
    "plt.plot(\n",
    "    np.sort(valve_train_moments.loc[:, \"Skewness\"]), valve_train_percentage, marker=\".\", linewidth=0\n",
    ")\n",
    "plt.plot([lim_skew[0], lim_skew[0]], plt.ylim(), color=\"k\")\n",
    "plt.plot([lim_skew[1], lim_skew[1]], plt.ylim(), color=\"k\")\n",
    "plt.xlabel(\"Value of the Skewness-percentile\")\n",
    "plt.ylabel(\"Percentile\")\n",
    "plt.subplot(2, 2, 4)\n",
    "plt.plot(\n",
    "    np.sort(valve_train_moments.loc[:, \"Kurtosis\"]), valve_train_percentage, marker=\".\", linewidth=0\n",
    ")\n",
    "plt.plot([limit_kurtosis[0], limit_kurtosis[0]], plt.ylim(), color=\"k\")\n",
    "plt.plot([limit_kurtosis[1], limit_kurtosis[1]], plt.ylim(), color=\"k\")\n",
    "plt.xlabel(\"Value of the Kurtosis-percentile\")\n",
    "plt.ylabel(\"Percentile\")\n",
    "\n",
    "# Define the full file path for the image\n",
    "file_path = os.path.join(output_dir, 'percentiles_moments_valve_train.png')\n",
    "\n",
    "# Save the plot to the subfolder\n",
    "plt.savefig(file_path, dpi=300)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36e634ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "quantiles = [0.2, 99.8]\n",
    "limit_mean = np.percentile(valve_train_moments.loc[:, \"Mean\"], quantiles)\n",
    "limit_variance = np.percentile(valve_train_moments.loc[:, \"Variance\"], quantiles)\n",
    "lim_skew = np.percentile(valve_train_moments.loc[:, \"Skewness\"], quantiles)\n",
    "limit_kurtosis = np.percentile(valve_train_moments.loc[:, \"Kurtosis\"], quantiles)\n",
    "\n",
    "valve_test_percentage = np.linspace(0, 100, valve_test_data_size[0])  # from 0% to 100% with step 100/nTrain\n",
    "\n",
    "plt.figure(2, figsize=(15, 10))\n",
    "plt.clf()\n",
    "plt.suptitle(\n",
    "    \"Fig.3: Percentiles of the 4 first centered moments for the valve \\n\" \n",
    "    + \"for both training and test sets with proposed thresholds.\",\n",
    "    fontsize=20,\n",
    ")\n",
    "plt.subplot(3, 2, 1)\n",
    "plt.plot(np.sort(valve_train_moments.loc[:, \"Mean\"]), valve_train_percentage, marker=\".\", linewidth=0)\n",
    "plt.plot(np.sort(valve_test_moments.loc[:, \"Mean\"]), valve_test_percentage, marker=\".\", linewidth=0)\n",
    "plt.plot([limit_mean[0], limit_mean[0]], plt.ylim(), color=\"k\")\n",
    "plt.plot([limit_mean[1], limit_mean[1]], plt.ylim(), color=\"k\")\n",
    "plt.xlabel(\"Mean\")\n",
    "plt.ylabel(\"Percentile\")\n",
    "\n",
    "plt.subplot(3, 2, 2)\n",
    "plt.plot(np.sort(valve_train_moments.loc[:, \"Variance\"]), valve_train_percentage, marker=\".\", linewidth=0)\n",
    "plt.plot(np.sort(valve_test_moments.loc[:, \"Variance\"]), valve_test_percentage, marker=\".\", linewidth=0)\n",
    "plt.plot([limit_variance[0], limit_variance[0]], plt.ylim(), color=\"k\")\n",
    "plt.plot([limit_variance[1], limit_variance[1]], plt.ylim(), color=\"k\")\n",
    "plt.xlabel(\"Variance\")\n",
    "plt.ylabel(\"Percentile\")\n",
    "\n",
    "plt.subplot(3, 2, 3)\n",
    "plt.plot(np.sort(valve_train_moments.loc[:, \"Skewness\"]), valve_train_percentage, marker=\".\", linewidth=0)\n",
    "plt.plot(np.sort(valve_test_moments.loc[:, \"Skewness\"]), valve_test_percentage, marker=\".\", linewidth=0)\n",
    "plt.plot([lim_skew[0], lim_skew[0]], plt.ylim(), color=\"k\")\n",
    "plt.plot([lim_skew[1], lim_skew[1]], plt.ylim(), color=\"k\")\n",
    "plt.xlabel(\"Skewness\")\n",
    "plt.ylabel(\"Percentile\")\n",
    "# plt.xlim([-50,50])\n",
    "\n",
    "plt.subplot(3, 2, 4)\n",
    "plt.plot(np.sort(valve_train_moments.loc[:, \"Kurtosis\"]), valve_train_percentage, marker=\".\", linewidth=0)\n",
    "plt.plot(np.sort(valve_test_moments.loc[:, \"Kurtosis\"]), valve_test_percentage, marker=\".\", linewidth=0)\n",
    "plt.plot([limit_kurtosis[0], limit_kurtosis[0]], plt.ylim(), color=\"k\")\n",
    "plt.plot([limit_kurtosis[1], limit_kurtosis[1]], plt.ylim(), color=\"k\")\n",
    "plt.xlabel(\"Kurtosis\")\n",
    "plt.ylabel(\"Percentile\")\n",
    "# plt.xlim([-50,5000])\n",
    "plt.figlegend(\n",
    "    [\"Train\", \"Test\", \"Threshold\"], loc=\"lower center\", ncol=5, labelspacing=0.0\n",
    ")\n",
    "\n",
    "# Define the full file path for the image\n",
    "file_path = os.path.join(output_dir, 'percentiles_moments_valve_train_and_test.png')\n",
    "\n",
    "# Save the plot to the subfolder\n",
    "plt.savefig(file_path, dpi=300)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b6a99b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select points either below 0.5 percentile or above 99.5 percentile of each moments and min-max limits\n",
    "abn_valve_train_mean = np.where(\n",
    "    (valve_train_moments.loc[:, \"Mean\"] < limit_mean[0])\n",
    "    | (valve_train_moments.loc[:, \"Mean\"] > limit_mean[1])\n",
    ")[0]\n",
    "abn_valve_train_variance = np.where(\n",
    "    (valve_train_moments.loc[:, \"Variance\"] < limit_variance[0])\n",
    "    | (valve_train_moments.loc[:, \"Variance\"] > limit_variance[1])\n",
    ")[0]\n",
    "abn_valve_train_skew = np.where(\n",
    "    (valve_train_moments.loc[:, \"Skewness\"] < lim_skew[0])\n",
    "    | (valve_train_moments.loc[:, \"Skewness\"] > lim_skew[1])\n",
    ")[0]\n",
    "abn_valve_train_kurtosis = np.where(\n",
    "    (valve_train_moments.loc[:, \"Kurtosis\"] < limit_kurtosis[0])\n",
    "    | (valve_train_moments.loc[:, \"Kurtosis\"] > limit_kurtosis[1])\n",
    ")[0]\n",
    "\n",
    "all_abnormalities_valve_train = np.unique(\n",
    "    np.concatenate(\n",
    "        (\n",
    "            abn_valve_train_mean,\n",
    "            abn_valve_train_variance,\n",
    "            abn_valve_train_skew,\n",
    "            abn_valve_train_kurtosis\n",
    "        )\n",
    "    )\n",
    ")\n",
    "\n",
    "# Compare sets together. Loop over each pair of sets and compute how many IDs they have in common\n",
    "valve_train_dict = {\n",
    "    \"Mean\": abn_valve_train_mean,\n",
    "    \"Variance\": abn_valve_train_variance,\n",
    "    \"Skewness\": abn_valve_train_skew,\n",
    "    \"Kurtsosis\": abn_valve_train_kurtosis\n",
    "}\n",
    "valve_train_dict_keys = list(valve_train_dict.keys())\n",
    "\n",
    "print(\"VALVE TRAINING DATASET\")\n",
    "for i in range(valve_train_dict.__len__()):\n",
    "    k1 = valve_train_dict_keys[i]\n",
    "    print(\"{}: {} anomalies found.\".format(k1, valve_train_dict[k1].__len__()))\n",
    "\n",
    "anomalies_count_valve_train = np.array(\n",
    "    [np.sum([i in valve_train_dict[k] for k in valve_train_dict]) for i in range(valve_train_data_size[0])]\n",
    ")\n",
    "for i in range(1, 5):\n",
    "    print(\n",
    "        \"{} samples are found as anomalous by at least {} selection methods.\".format(\n",
    "            len(np.where(anomalies_count_valve_train >= i)[0]), i\n",
    "        )\n",
    "    )\n",
    "\n",
    "######################################################################################################\n",
    "\n",
    "# select points either below 0.2 percentile or above 99.8 percentile of each moments and min-max limits\n",
    "abn_valve_test_mean = np.where(\n",
    "    (valve_test_moments.loc[:, \"Mean\"] < limit_mean[0])\n",
    "    | (valve_test_moments.loc[:, \"Mean\"] > limit_mean[1])\n",
    ")[0]\n",
    "abn_valve_test_variance = np.where(\n",
    "    (valve_test_moments.loc[:, \"Variance\"] < limit_variance[0])\n",
    "    | (valve_test_moments.loc[:, \"Variance\"] > limit_variance[1])\n",
    ")[0]\n",
    "abn_valve_test_skew = np.where(\n",
    "    (valve_test_moments.loc[:, \"Skewness\"] < lim_skew[0])\n",
    "    | (valve_test_moments.loc[:, \"Skewness\"] > lim_skew[1])\n",
    ")[0]\n",
    "abn_valve_test_kurtosis = np.where(\n",
    "    (valve_test_moments.loc[:, \"Kurtosis\"] < limit_kurtosis[0])\n",
    "    | (valve_test_moments.loc[:, \"Kurtosis\"] > limit_kurtosis[1])\n",
    ")[0]\n",
    "\n",
    "allAbnormalities = np.unique(\n",
    "    np.concatenate(\n",
    "        (\n",
    "            abn_valve_test_mean,\n",
    "            abn_valve_test_variance,\n",
    "            abn_valve_test_skew,\n",
    "            abn_valve_test_kurtosis\n",
    "    )\n",
    ")\n",
    ")\n",
    "\n",
    "\n",
    "# compare sets together. Here I loop over each pair of set and compute how many ID they have in common\n",
    "valve_test_dict = {\n",
    "    \"Mean\": abn_valve_test_mean,\n",
    "    \"Variance\": abn_valve_test_variance,\n",
    "    \"Skewness\": abn_valve_test_skew,\n",
    "    \"Kurtosis\": abn_valve_test_kurtosis\n",
    "}\n",
    "keysTest = list(valve_test_dict.keys())\n",
    "\n",
    "\n",
    "print(\"\\nVALVE TEST DATASET\")\n",
    "for ii in range(valve_test_dict.__len__()):\n",
    "    k1 = keysTest[ii]\n",
    "    print(f\"{k1}: {len(valve_test_dict[k1])} anomalies found.\")\n",
    "\n",
    "countAbnormalTest = np.array(\n",
    "    [np.sum([i in valve_test_dict[k] for k in valve_test_dict]) for i in range(valve_test_data_size[0])]\n",
    ")\n",
    "for i in range(1, 5):\n",
    "    print(\n",
    "        f\"{len(np.where(countAbnormalTest >= i)[0])} samples are found as anomalous \"\n",
    "        f\"by at least {i} selection methods.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90643016",
   "metadata": {},
   "source": [
    "#### Pump ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1717ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "quantiles = [0.5, 99.5]\n",
    "limit_mean = np.percentile(pump_train_moments.loc[:, \"Mean\"], quantiles)\n",
    "limit_variance = np.percentile(pump_train_moments.loc[:, \"Variance\"], quantiles)\n",
    "lim_skew = np.percentile(pump_train_moments.loc[:, \"Skewness\"], quantiles)\n",
    "limit_kurtosis = np.percentile(pump_train_moments.loc[:, \"Kurtosis\"], quantiles)\n",
    "pump_train_percentage = np.linspace(0, 100, pump_train_data_size[0])  # From 0% to 100%\n",
    "\n",
    "\n",
    "#Plotting the percentiles of mean, variance, skew and kurtosis for the valve training set\n",
    "plt.figure(1, figsize=(15, 8))\n",
    "plt.clf()\n",
    "plt.suptitle(\n",
    "    \"Fig.1: Percentiles of the 4 first centered Moment for the training set for the pump.\",\n",
    "    fontsize=20,\n",
    ")\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.plot(\n",
    "    np.sort(pump_train_moments.loc[:, \"Mean\"]), pump_train_percentage, marker=\".\", linewidth=0\n",
    ")\n",
    "plt.plot([limit_mean[0], limit_mean[0]], plt.ylim(), color=\"k\")\n",
    "plt.plot([limit_mean[1], limit_mean[1]], plt.ylim(), color=\"k\")\n",
    "plt.xlabel(\"Value of the Mean-percentile\")\n",
    "plt.ylabel(\"Percentile\")\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.plot(\n",
    "    np.sort(pump_train_moments.loc[:, \"Variance\"]), pump_train_percentage, marker=\".\", linewidth=0\n",
    ")\n",
    "plt.plot([limit_variance[0], limit_variance[0]], plt.ylim(), color=\"k\")\n",
    "plt.plot([limit_variance[1], limit_variance[1]], plt.ylim(), color=\"k\")\n",
    "plt.xlabel(\"Value of the Variance-percentile\")\n",
    "plt.ylabel(\"Percentile\")\n",
    "plt.subplot(2, 2, 3)\n",
    "plt.plot(\n",
    "    np.sort(pump_train_moments.loc[:, \"Skewness\"]), pump_train_percentage, marker=\".\", linewidth=0\n",
    ")\n",
    "plt.plot([lim_skew[0], lim_skew[0]], plt.ylim(), color=\"k\")\n",
    "plt.plot([lim_skew[1], lim_skew[1]], plt.ylim(), color=\"k\")\n",
    "plt.xlabel(\"Value of the Skewness-percentile\")\n",
    "plt.ylabel(\"Percentile\")\n",
    "plt.subplot(2, 2, 4)\n",
    "plt.plot(\n",
    "    np.sort(pump_train_moments.loc[:, \"Kurtosis\"]), pump_train_percentage, marker=\".\", linewidth=0\n",
    ")\n",
    "plt.plot([limit_kurtosis[0], limit_kurtosis[0]], plt.ylim(), color=\"k\")\n",
    "plt.plot([limit_kurtosis[1], limit_kurtosis[1]], plt.ylim(), color=\"k\")\n",
    "plt.xlabel(\"Value of the Kurtosis-percentile\")\n",
    "plt.ylabel(\"Percentile\")\n",
    "\n",
    "# Define the full file path for the image\n",
    "file_path = os.path.join(output_dir, 'percentiles_moments_pump_train.png')\n",
    "\n",
    "# Save the plot to the subfolder\n",
    "plt.savefig(file_path, dpi=300)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d122d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "quantiles = [0.2, 99.8]\n",
    "limit_mean = np.percentile(pump_train_moments.loc[:, \"Mean\"], quantiles)\n",
    "limit_variance = np.percentile(pump_train_moments.loc[:, \"Variance\"], quantiles)\n",
    "lim_skew = np.percentile(pump_train_moments.loc[:, \"Skewness\"], quantiles)\n",
    "limit_kurtosis = np.percentile(pump_train_moments.loc[:, \"Kurtosis\"], quantiles)\n",
    "\n",
    "pump_test_percentage = np.linspace(0, 100, pump_test_data_size[0])  # from 0% to 100% with step 100/nTrain\n",
    "\n",
    "plt.figure(2, figsize=(15, 10))\n",
    "plt.clf()\n",
    "plt.suptitle(\n",
    "    \"Fig.3: Percentiles of the 4 first centered moments for the pump \\n\"\n",
    "    + \"for both training and test sets with proposed thresholds.\",\n",
    "    fontsize=20,\n",
    ")\n",
    "plt.subplot(3, 2, 1)\n",
    "plt.plot(np.sort(pump_train_moments.loc[:, \"Mean\"]), pump_train_percentage, marker=\".\", linewidth=0)\n",
    "plt.plot(np.sort(pump_test_moments.loc[:, \"Mean\"]), pump_test_percentage, marker=\".\", linewidth=0)\n",
    "plt.plot([limit_mean[0], limit_mean[0]], plt.ylim(), color=\"k\")\n",
    "plt.plot([limit_mean[1], limit_mean[1]], plt.ylim(), color=\"k\")\n",
    "plt.xlabel(\"Mean\")\n",
    "plt.ylabel(\"Percentile\")\n",
    "\n",
    "plt.subplot(3, 2, 2)\n",
    "plt.plot(np.sort(pump_train_moments.loc[:, \"Variance\"]), pump_train_percentage, marker=\".\", linewidth=0)\n",
    "plt.plot(np.sort(pump_test_moments.loc[:, \"Variance\"]), pump_test_percentage, marker=\".\", linewidth=0)\n",
    "plt.plot([limit_variance[0], limit_variance[0]], plt.ylim(), color=\"k\")\n",
    "plt.plot([limit_variance[1], limit_variance[1]], plt.ylim(), color=\"k\")\n",
    "plt.xlabel(\"Variance\")\n",
    "plt.ylabel(\"Percentile\")\n",
    "\n",
    "plt.subplot(3, 2, 3)\n",
    "plt.plot(np.sort(pump_train_moments.loc[:, \"Skewness\"]), pump_train_percentage, marker=\".\", linewidth=0)\n",
    "plt.plot(np.sort(pump_test_moments.loc[:, \"Skewness\"]), pump_test_percentage, marker=\".\", linewidth=0)\n",
    "plt.plot([lim_skew[0], lim_skew[0]], plt.ylim(), color=\"k\")\n",
    "plt.plot([lim_skew[1], lim_skew[1]], plt.ylim(), color=\"k\")\n",
    "plt.xlabel(\"Skewness\")\n",
    "plt.ylabel(\"Percentile\")\n",
    "# plt.xlim([-50,50])\n",
    "\n",
    "plt.subplot(3, 2, 4)\n",
    "plt.plot(np.sort(pump_train_moments.loc[:, \"Kurtosis\"]), pump_train_percentage, marker=\".\", linewidth=0)\n",
    "plt.plot(np.sort(pump_test_moments.loc[:, \"Kurtosis\"]), pump_test_percentage, marker=\".\", linewidth=0)\n",
    "plt.plot([limit_kurtosis[0], limit_kurtosis[0]], plt.ylim(), color=\"k\")\n",
    "plt.plot([limit_kurtosis[1], limit_kurtosis[1]], plt.ylim(), color=\"k\")\n",
    "plt.xlabel(\"Kurtosis\")\n",
    "plt.ylabel(\"Percentile\")\n",
    "# plt.xlim([-50,5000])\n",
    "plt.figlegend(\n",
    "    [\"Train\", \"Test\", \"Threshold\"], loc=\"lower center\", ncol=5, labelspacing=0.0\n",
    ")\n",
    "\n",
    "# Define the full file path for the image\n",
    "file_path = os.path.join(output_dir, 'percentiles_moments_pump_train_and_test.png')\n",
    "\n",
    "# Save the plot to the subfolder\n",
    "plt.savefig(file_path, dpi=300)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70f6536d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select points either below 0.5 percentile or above 99.5 percentile of each moments and min-max limits\n",
    "abn_pump_train_mean = np.where(\n",
    "    (pump_train_moments.loc[:, \"Mean\"] < limit_mean[0])\n",
    "    | (pump_train_moments.loc[:, \"Mean\"] > limit_mean[1])\n",
    ")[0]\n",
    "abn_pump_train_variance = np.where(\n",
    "    (pump_train_moments.loc[:, \"Variance\"] < limit_variance[0])\n",
    "    | (pump_train_moments.loc[:, \"Variance\"] > limit_variance[1])\n",
    ")[0]\n",
    "abn_pump_train_skew = np.where(\n",
    "    (pump_train_moments.loc[:, \"Skewness\"] < lim_skew[0])\n",
    "    | (pump_train_moments.loc[:, \"Skewness\"] > lim_skew[1])\n",
    ")[0]\n",
    "abn_pump_train_kurtosis = np.where(\n",
    "    (pump_train_moments.loc[:, \"Kurtosis\"] < limit_kurtosis[0])\n",
    "    | (pump_train_moments.loc[:, \"Kurtosis\"] > limit_kurtosis[1])\n",
    ")[0]\n",
    "\n",
    "all_abnormalities_pump_train = np.unique(\n",
    "    np.concatenate(\n",
    "        (\n",
    "            abn_pump_train_mean,\n",
    "            abn_pump_train_variance,\n",
    "            abn_pump_train_skew,\n",
    "            abn_pump_train_kurtosis\n",
    "        )\n",
    "    )\n",
    ")\n",
    "\n",
    "# Compare sets together. Loop over each pair of sets and compute how many IDs they have in common\n",
    "pump_train_dict = {\n",
    "    \"Mean\": abn_pump_train_mean,\n",
    "    \"Variance\": abn_pump_train_variance,\n",
    "    \"Skewness\": abn_pump_train_skew,\n",
    "    \"Kurtsosis\": abn_pump_train_kurtosis\n",
    "}\n",
    "pump_train_dict_keys = list(pump_train_dict.keys())\n",
    "\n",
    "print(\"PUMP TRAINING DATASET\")\n",
    "for i in range(pump_train_dict.__len__()):\n",
    "    k1 = pump_train_dict_keys[i]\n",
    "    print(\"{}: {} anomalies found.\".format(k1, pump_train_dict[k1].__len__()))\n",
    "\n",
    "anomalies_count_pump_train = np.array(\n",
    "    [np.sum([i in pump_train_dict[k] for k in pump_train_dict]) for i in range(pump_train_data_size[0])]\n",
    ")\n",
    "for i in range(1, 5):\n",
    "    print(\n",
    "        \"{} samples are found as anomalous by at least {} selection methods.\".format(\n",
    "            len(np.where(anomalies_count_pump_train >= i)[0]), i\n",
    "        )\n",
    "    )\n",
    "\n",
    "######################################################################################################\n",
    "\n",
    "# select points either below 0.2 percentile or above 99.8 percentile of each moments and min-max limits\n",
    "abn_pump_test_mean = np.where(\n",
    "    (pump_test_moments.loc[:, \"Mean\"] < limit_mean[0])\n",
    "    | (pump_test_moments.loc[:, \"Mean\"] > limit_mean[1])\n",
    ")[0]\n",
    "abn_pump_test_variance = np.where(\n",
    "    (pump_test_moments.loc[:, \"Variance\"] < limit_variance[0])\n",
    "    | (pump_test_moments.loc[:, \"Variance\"] > limit_variance[1])\n",
    ")[0]\n",
    "abn_pump_test_skew = np.where(\n",
    "    (pump_test_moments.loc[:, \"Skewness\"] < lim_skew[0])\n",
    "    | (pump_test_moments.loc[:, \"Skewness\"] > lim_skew[1])\n",
    ")[0]\n",
    "abn_pump_test_kurtosis = np.where(\n",
    "    (pump_test_moments.loc[:, \"Kurtosis\"] < limit_kurtosis[0])\n",
    "    | (pump_test_moments.loc[:, \"Kurtosis\"] > limit_kurtosis[1])\n",
    ")[0]\n",
    "\n",
    "allAbnormalities = np.unique(\n",
    "    np.concatenate(\n",
    "        (\n",
    "            abn_pump_test_mean,\n",
    "            abn_pump_test_variance,\n",
    "            abn_pump_test_skew,\n",
    "            abn_pump_test_kurtosis\n",
    "    )\n",
    ")\n",
    ")\n",
    "\n",
    "\n",
    "# compare sets together. Here I loop over each pair of set and compute how many ID they have in common\n",
    "pump_test_dict = {\n",
    "    \"Mean\": abn_pump_test_mean,\n",
    "    \"Variance\": abn_pump_test_variance,\n",
    "    \"Skewness\": abn_pump_test_skew,\n",
    "    \"Kurtosis\": abn_pump_test_kurtosis\n",
    "}\n",
    "keysTest = list(pump_test_dict.keys())\n",
    "\n",
    "\n",
    "print(\"PUMP TEST DATASET\")\n",
    "for ii in range(pump_test_dict.__len__()):\n",
    "    k1 = keysTest[ii]\n",
    "    print(f\"{k1}: {len(pump_test_dict[k1])} anomalies found.\")\n",
    "\n",
    "countAbnormalTest = np.array(\n",
    "    [np.sum([i in pump_test_dict[k] for k in pump_test_dict]) for i in range(pump_test_data_size[0])]\n",
    ")\n",
    "for i in range(1, 5):\n",
    "    print(\n",
    "        f\"{len(np.where(countAbnormalTest >= i)[0])} samples are found as anomalous \"\n",
    "        f\"by at least {i} selection methods.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd1ed8a7",
   "metadata": {},
   "source": [
    "### Question 4:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "c9146707",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_features_old(signal, sr=16000):\n",
    "    # Compute Short-Time Fourier Transform (STFT)\n",
    "    stft = np.abs(librosa.stft(signal, n_fft=1024, hop_length=512, window='hann'))  # Short-Time Fourier Transform (Magnitude)\n",
    "    \n",
    "    # Compute Power Spectral Density (PSD)\n",
    "    psd = stft ** 2\n",
    "    psd /= np.sum(psd, axis=0, keepdims=True)  # Normalize along time axis\n",
    "    \n",
    "    # Frequency bins corresponding to STFT\n",
    "    freqs = librosa.fft_frequencies(sr=sr, n_fft=1024)\n",
    "\n",
    "    # Compute Spectral Entropy (Mean across frames)\n",
    "    spectral_entropy = -np.sum(psd * np.log2(psd + 1e-10), axis=0)\n",
    "    spectral_entropy_mean = np.mean(spectral_entropy)\n",
    "\n",
    "    # Compute Spectral Centroid (Mean and Std)\n",
    "    spectral_centroid = librosa.feature.spectral_centroid(S=stft, sr=sr)\n",
    "    spectral_centroid_mean = np.mean(spectral_centroid)\n",
    "    spectral_centroid_std = np.std(spectral_centroid)\n",
    "    # Compute spectral contrast (No good result)\n",
    "    spectral_contrast = np.mean(librosa.feature.spectral_contrast(S=stft, sr=sr), axis=1)\n",
    "    spectral_contrast_mean = np.mean(spectral_contrast)\n",
    "    spectral_contrast_std = np.std(spectral_contrast)\n",
    "\n",
    "\n",
    "    # Energy Ratios\n",
    "    low_energy = np.sum(psd[freqs <= 500, :], axis=0)  # Low frequency energy\n",
    "    high_energy = np.sum(psd[freqs > 3000, :], axis=0)  # High frequency energy\n",
    "    energy_ratio = np.mean(high_energy / (low_energy + 1e-10))  # Mean ratio across frames\n",
    "    \n",
    "    # New energie version tunning the ratio to see if there is better result --> no\n",
    "    low_energy = np.sum(psd[freqs <= 1000])  # Energy below 1000 Hz\n",
    "    mid_energy = np.sum(psd[(freqs > 1000) & (freqs <= 3000)])  # Energy between 1000-3000 Hz\n",
    "    high_energy = np.sum(psd[freqs > 3000])  # Energy above 3000 Hz\n",
    "    energy_ratio = high_energy / (low_energy + 1e-10)\n",
    "\n",
    "    # Calculate standard deviation of amplitude values across time for each frequency band\n",
    "    spec_db = 20 * np.log10(stft + 1e-10)  # Convert to dB scale to prevent log(0)\n",
    "    std_amplitude = np.mean(np.std(spec_db, axis=1))  # Std across time, then mean over frequency bands\n",
    "\n",
    "    # Peak prominence in the FFT domain\n",
    "    fft_values = np.abs(np.fft.rfft(signal)) # real fast fourrier transform \n",
    "    fft_freqs = np.fft.rfftfreq(len(signal), 1/sr)\n",
    "    peak_2000 = max(fft_values[(fft_freqs >= 2000) & (fft_freqs <= 2500)]) if any(fft_values[(fft_freqs >= 2000) & (fft_freqs <= 2500)]) else 0 # if block to handle error\n",
    "\n",
    "    # Peak analysis using FFT for prominent peaks\n",
    "    peaks, _ = find_peaks(fft_values, prominence=10)  # Find peaks with prominence threshold\n",
    "    num_peaks = len(peaks)\n",
    "\n",
    "    # Autocorrelation feature \n",
    "    autocorrelation = librosa.autocorrelate(signal) \n",
    "    # \"Maximum value: Represents the highest similarity\"\n",
    "    autocorrelation_max = np.max(autocorrelation)\n",
    "    # Captures the average behavior. \n",
    "    autocorrelation_mean = np.mean(autocorrelation)\n",
    "    autocorrelation_std = np.std(autocorrelation)\n",
    "    # representing periodicity\n",
    "    first_peak = np.argmax(autocorrelation[1:]) + 1\n",
    "    # eflect signal smoothness.\n",
    "    energy_decay = np.sum(np.abs(autocorrelation))\n",
    "\n",
    "    # each of the autocorrealtion featues are used in the autocorrelation column and only the most relevant are keeped\n",
    "    # max, mean are ok\n",
    "    # std, energy_decay are bad\n",
    "    # first peak is useless\n",
    "\n",
    "    # Harmonic to Noise Ratio (HNR) -> Bad\n",
    "    harmonic, percussive = librosa.effects.hpss(signal)\n",
    "    hnr = np.mean(librosa.feature.rms(y=harmonic) / (librosa.feature.rms(y=percussive) + 1e-10))\n",
    "\n",
    "    # Spectral flatterness -> ok\n",
    "    spectral_flatness = np.mean(librosa.feature.spectral_flatness(y=signal))\n",
    "\n",
    "    # Zero crossing rate -> ok\n",
    "    zero_crossings = np.mean(librosa.feature.zero_crossing_rate(y=signal))\n",
    "\n",
    "    # Spectral Bandwidth -> almost good \n",
    "    spectral_bandwidth = np.mean(librosa.feature.spectral_bandwidth(S=stft, sr=sr))\n",
    "    # tuning this feature\n",
    "    # Spectral Bandwidth Features\n",
    "    spectral_bandwidth_values = librosa.feature.spectral_bandwidth(S=stft, sr=sr)\n",
    "    spectral_bandwidth_mean = np.mean(spectral_bandwidth_values)\n",
    "    spectral_bandwidth_std = np.std(spectral_bandwidth_values)\n",
    "    spectral_bandwidth_skewness = scist.skew(spectral_bandwidth_values, axis=None)\n",
    "    spectral_bandwidth_kurtosis = scist.kurtosis(spectral_bandwidth_values, axis=None)\n",
    "    mean_std_ratio = spectral_bandwidth_mean / (spectral_bandwidth_std + 1e-10)\n",
    "    max_mean_ratio = np.max(spectral_bandwidth_values) / (spectral_bandwidth_mean + 1e-10)\n",
    "    spectral_bandwidth_diff = np.diff(spectral_bandwidth_values, axis=1)\n",
    "    change_rate = np.mean(np.abs(spectral_bandwidth_diff))  # Rate of change\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    return {\n",
    "        \"num_peaks\": num_peaks,\n",
    "        \"spectral_entropy_mean\": spectral_entropy_mean,\n",
    "        \"spectral_centroid_mean\": spectral_centroid_mean,\n",
    "        \"spectral_centroid_std\": spectral_centroid_std,\n",
    "        \"energy_ratio\": energy_ratio,\n",
    "        \"std_spectogram\": std_amplitude,\n",
    "        \"peak_2000_prominence\": peak_2000,\n",
    "        \"spectral_bandwidth\": spectral_bandwidth_mean,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "f035a727",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_features(signal, sr=16000):\n",
    "    # Compute Short-Time Fourier Transform (STFT)\n",
    "    stft = np.abs(librosa.stft(signal, n_fft=1024, hop_length=512, window='hann'))\n",
    "\n",
    "    # Compute Power Spectral Density (PSD) and frequency bins\n",
    "    psd = stft ** 2\n",
    "    psd /= np.sum(psd, axis=0, keepdims=True)  # Normalize across time frames\n",
    "    freqs = librosa.fft_frequencies(sr=sr, n_fft=1024)\n",
    "\n",
    "    # Compute Spectral Entropy (Mean across frames)\n",
    "    spectral_entropy = -np.sum(psd * np.log2(psd + 1e-10), axis=0)\n",
    "    spectral_entropy_mean = np.mean(spectral_entropy)\n",
    "\n",
    "    # Compute Spectral Centroid (Mean and Std)\n",
    "    spectral_centroid = librosa.feature.spectral_centroid(S=stft, sr=sr)\n",
    "    spectral_centroid_mean = np.mean(spectral_centroid)\n",
    "    spectral_centroid_std = np.std(spectral_centroid)\n",
    "\n",
    "    # Energy Ratio (High vs. Low Frequency)\n",
    "    low_energy = np.sum(psd[freqs <= 500, :], axis=0)\n",
    "    high_energy = np.sum(psd[freqs > 3000, :], axis=0)\n",
    "    energy_ratio = np.mean(high_energy / (low_energy + 1e-10))\n",
    "\n",
    "    # Standard Deviation of Amplitude Values (dB)\n",
    "    spec_db = 20 * np.log10(stft + 1e-10)  # Convert to dB scale\n",
    "    std_amplitude = np.mean(np.std(spec_db, axis=1))\n",
    "\n",
    "    # Peak prominence in FFT domain from 2000 Hz to 2500 Hz\n",
    "    fft_values = np.abs(np.fft.rfft(signal))\n",
    "    fft_freqs = np.fft.rfftfreq(len(signal), 1/sr)\n",
    "    peak_2000 = max(fft_values[(fft_freqs >= 2150) & (fft_freqs <= 2350)]) if any(fft_values[(fft_freqs >= 2150) & (fft_freqs <= 2350)]) else 0\n",
    "\n",
    "\n",
    "    # Count prominent peaks in the FFT\n",
    "    peaks, _ = find_peaks(fft_values, prominence=10)\n",
    "    num_peaks = len(peaks)\n",
    "\n",
    "    # Spectral Bandwidth (captures spread of frequencies)\n",
    "    spectral_bandwidth_values = librosa.feature.spectral_bandwidth(S=stft, sr=sr)\n",
    "    spectral_bandwidth_mean = np.mean(spectral_bandwidth_values)\n",
    "    \n",
    "\n",
    "    # Return a dictionary with the selected features\n",
    "    return {\n",
    "        \"num_peaks\": num_peaks,\n",
    "        \"spectral_entropy_mean\": spectral_entropy_mean,\n",
    "        \"spectral_centroid_mean\": spectral_centroid_mean,\n",
    "        \"spectral_centroid_std\": spectral_centroid_std,\n",
    "        \"energy_ratio\": energy_ratio,\n",
    "        \"std_spectogram\": std_amplitude,\n",
    "        \"peak_2000_prominence\": peak_2000,\n",
    "        \"spectral_bandwidth\": spectral_bandwidth_mean,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be4a018e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataFrames for each dataset\n",
    "def create_feature_dataframe(data):\n",
    "    # Calculate features for each sample (row)\n",
    "    features = [calculate_features(data[i]) for i in range(data.shape[0])]\n",
    "    return pd.DataFrame(features)\n",
    "\n",
    "# Generate DataFrames\n",
    "pump_train_df = create_feature_dataframe(pump_train_data)\n",
    "pump_test_df = create_feature_dataframe(pump_test_data)\n",
    "\n",
    "print(\"\\nPump Train DataFrame:\")\n",
    "print(pump_train_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4118d689",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the quantiles for threshold lines\n",
    "quantiles = [0.2, 99.8]\n",
    "\n",
    "# Calculate limits for each feature\n",
    "limit_num_peaks = np.percentile(pump_train_df['num_peaks'], quantiles)\n",
    "limit_spectral_entropy = np.percentile(pump_train_df['spectral_entropy_mean'], quantiles)\n",
    "limit_spectral_centroid_mean = np.percentile(pump_train_df['spectral_centroid_mean'], quantiles)\n",
    "limit_spectral_centroid_std = np.percentile(pump_train_df['spectral_centroid_std'], quantiles)\n",
    "limit_energy_ratio = np.percentile(pump_train_df['energy_ratio'], quantiles)\n",
    "limit_std_spectogram = np.percentile(pump_train_df['std_spectogram'], quantiles)\n",
    "limit_peak_2000 = np.percentile(pump_train_df['peak_2000_prominence'], quantiles)\n",
    "limit_spectral_bandwidth = np.percentile(pump_train_df['spectral_bandwidth'], quantiles)\n",
    "\n",
    "\n",
    "# Get percentage for Pump Test Data\n",
    "pump_train_percentage = np.linspace(0, 100, pump_train_df.shape[0])\n",
    "pump_test_percentage = np.linspace(0, 100, pump_test_df.shape[0])\n",
    "\n",
    "# Set up the matplotlib figure\n",
    "plt.figure(figsize=(15, 12))\n",
    "plt.clf()\n",
    "plt.suptitle(\n",
    "    \"Percentiles of Features for Pump Train and Test Sets with Proposed Thresholds\",\n",
    "    fontsize=20,\n",
    ")\n",
    "\n",
    "# Plot for num_peaks\n",
    "plt.subplot(4, 2, 1)\n",
    "plt.plot(np.sort(pump_train_df['num_peaks']), pump_train_percentage, marker='.', linewidth=0)\n",
    "plt.plot(np.sort(pump_test_df['num_peaks']), pump_test_percentage, marker='.', linewidth=0)\n",
    "plt.plot([limit_num_peaks[0], limit_num_peaks[0]], plt.ylim(), color='k')\n",
    "plt.plot([limit_num_peaks[1], limit_num_peaks[1]], plt.ylim(), color='k')\n",
    "plt.xlabel(\"Number of Peaks\")\n",
    "plt.ylabel(\"Percentile\")\n",
    "plt.title(\"Num Peaks\")\n",
    "\n",
    "# Plot for spectral_entropy_mean\n",
    "plt.subplot(4, 2, 2)\n",
    "plt.plot(np.sort(pump_train_df['spectral_entropy_mean']), pump_train_percentage, marker='.', linewidth=0)\n",
    "plt.plot(np.sort(pump_test_df['spectral_entropy_mean']), pump_test_percentage, marker='.', linewidth=0)\n",
    "plt.plot([limit_spectral_entropy[0], limit_spectral_entropy[0]], plt.ylim(), color='k')\n",
    "plt.plot([limit_spectral_entropy[1], limit_spectral_entropy[1]], plt.ylim(), color='k')\n",
    "plt.xlabel(\"Spectral Entropy Mean\")\n",
    "plt.ylabel(\"Percentile\")\n",
    "plt.title(\"Spectral Entropy Mean\")\n",
    "\n",
    "# Plot for spectral_centroid_mean\n",
    "plt.subplot(4, 2, 3)\n",
    "plt.plot(np.sort(pump_train_df['spectral_centroid_mean']), pump_train_percentage, marker='.', linewidth=0)\n",
    "plt.plot(np.sort(pump_test_df['spectral_centroid_mean']), pump_test_percentage, marker='.', linewidth=0)\n",
    "plt.plot([limit_spectral_centroid_mean[0], limit_spectral_centroid_mean[0]], plt.ylim(), color='k')\n",
    "plt.plot([limit_spectral_centroid_mean[1], limit_spectral_centroid_mean[1]], plt.ylim(), color='k')\n",
    "plt.xlabel(\"Spectral Centroid Mean\")\n",
    "plt.ylabel(\"Percentile\")\n",
    "plt.title(\"Spectral Centroid Mean\")\n",
    "\n",
    "# Plot for spectral_centroid_std\n",
    "plt.subplot(4, 2, 4)\n",
    "plt.plot(np.sort(pump_train_df['spectral_centroid_std']), pump_train_percentage, marker='.', linewidth=0)\n",
    "plt.plot(np.sort(pump_test_df['spectral_centroid_std']), pump_test_percentage, marker='.', linewidth=0)\n",
    "plt.plot([limit_spectral_centroid_std[0], limit_spectral_centroid_std[0]], plt.ylim(), color='k')\n",
    "plt.plot([limit_spectral_centroid_std[1], limit_spectral_centroid_std[1]], plt.ylim(), color='k')\n",
    "plt.xlabel(\"Spectral Centroid Std\")\n",
    "plt.ylabel(\"Percentile\")\n",
    "plt.title(\"Spectral Centroid Std\")\n",
    "\n",
    "# Plot for energy_ratio\n",
    "plt.subplot(4, 2, 5)\n",
    "plt.plot(np.sort(pump_train_df['energy_ratio']), pump_train_percentage, marker='.', linewidth=0)\n",
    "plt.plot(np.sort(pump_test_df['energy_ratio']), pump_test_percentage, marker='.', linewidth=0)\n",
    "plt.plot([limit_energy_ratio[0], limit_energy_ratio[0]], plt.ylim(), color='k')\n",
    "plt.plot([limit_energy_ratio[1], limit_energy_ratio[1]], plt.ylim(), color='k')\n",
    "plt.xlabel(\"Energy Ratio\")\n",
    "plt.ylabel(\"Percentile\")\n",
    "plt.title(\"Energy Ratio\")\n",
    "\n",
    "# Plot for std spectogrgam\n",
    "plt.subplot(4, 2, 6)\n",
    "plt.plot(np.sort(pump_train_df['std_spectogram']), pump_train_percentage, marker='.', linewidth=0)\n",
    "plt.plot(np.sort(pump_test_df['std_spectogram']), pump_test_percentage, marker='.', linewidth=0)\n",
    "plt.plot([limit_std_spectogram[0], limit_std_spectogram[0]], plt.ylim(), color='k')\n",
    "plt.plot([limit_std_spectogram[1], limit_std_spectogram[1]], plt.ylim(), color='k')\n",
    "plt.xlabel(\"Standard Deviation of the spectogram\")\n",
    "plt.ylabel(\"Percentile\")\n",
    "plt.title(\"Standard Deviation of the spectogram\")\n",
    "\n",
    "# Plot for peak_2000_prominence\n",
    "plt.subplot(4, 2, 7)\n",
    "plt.plot(np.sort(pump_train_df['peak_2000_prominence']), pump_train_percentage, marker='.', linewidth=0)\n",
    "plt.plot(np.sort(pump_test_df['peak_2000_prominence']), pump_test_percentage, marker='.', linewidth=0)\n",
    "plt.plot([limit_peak_2000[0], limit_peak_2000[0]], plt.ylim(), color='k')\n",
    "plt.plot([limit_peak_2000[1], limit_peak_2000[1]], plt.ylim(), color='k')\n",
    "plt.xlabel(\"Peak at 2000 Hz Prominence\")\n",
    "plt.ylabel(\"Percentile\")\n",
    "plt.title(\"Peak 2000 Hz Prominence\")\n",
    "\n",
    "# Plot for Spectral Bandwidth\n",
    "plt.subplot(4, 2, 8)\n",
    "plt.plot(np.sort(pump_train_df['spectral_bandwidth']), pump_train_percentage, marker='.', linewidth=0)\n",
    "plt.plot(np.sort(pump_test_df['spectral_bandwidth']), pump_test_percentage, marker='.', linewidth=0)\n",
    "plt.plot([limit_spectral_bandwidth[0], limit_spectral_bandwidth[0]], plt.ylim(), color='k')\n",
    "plt.plot([limit_spectral_bandwidth[1], limit_spectral_bandwidth[1]], plt.ylim(), color='k')\n",
    "plt.xlabel(\"Spectral Bandwidth\")\n",
    "plt.ylabel(\"Percentile\")\n",
    "plt.title(\"Spectral Bandwidth\")\n",
    "\n",
    "# Legend\n",
    "plt.figlegend([\"Pump Train\", \"Pump Test\", \"Threshold\"], loc=\"lower center\", ncol=3, labelspacing=0.0)\n",
    "\n",
    "# Show the plot\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.95])  # Adjust layout to fit the title\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1018538",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Detect anomalies in the training dataset\n",
    "abn_num_peaks = np.where(\n",
    "    (pump_train_df['num_peaks'] < limit_num_peaks[0]) |\n",
    "    (pump_train_df['num_peaks'] > limit_num_peaks[1])\n",
    ")[0]\n",
    "\n",
    "abn_spectral_entropy = np.where(\n",
    "    (pump_train_df['spectral_entropy_mean'] < limit_spectral_entropy[0]) |\n",
    "    (pump_train_df['spectral_entropy_mean'] > limit_spectral_entropy[1])\n",
    ")[0]\n",
    "\n",
    "abn_spectral_centroid_mean = np.where(\n",
    "    (pump_train_df['spectral_centroid_mean'] < limit_spectral_centroid_mean[0]) |\n",
    "    (pump_train_df['spectral_centroid_mean'] > limit_spectral_centroid_mean[1])\n",
    ")[0]\n",
    "\n",
    "abn_spectral_centroid_std = np.where(\n",
    "    (pump_train_df['spectral_centroid_std'] < limit_spectral_centroid_std[0]) |\n",
    "    (pump_train_df['spectral_centroid_std'] > limit_spectral_centroid_std[1])\n",
    ")[0]\n",
    "\n",
    "abn_energy_ratio = np.where(\n",
    "    (pump_train_df['energy_ratio'] < limit_energy_ratio[0]) |\n",
    "    (pump_train_df['energy_ratio'] > limit_energy_ratio[1])\n",
    ")[0]\n",
    "\n",
    "abn_std_spectrogram = np.where(\n",
    "    (pump_train_df['std_spectogram'] < limit_std_spectogram[0]) |\n",
    "    (pump_train_df['std_spectogram'] > limit_std_spectogram[1])\n",
    ")[0]\n",
    "\n",
    "abn_peak_2000 = np.where(\n",
    "    (pump_train_df['peak_2000_prominence'] < limit_peak_2000[0]) |\n",
    "    (pump_train_df['peak_2000_prominence'] > limit_peak_2000[1])\n",
    ")[0]\n",
    "\n",
    "abn_spectral_bandwidth = np.where(\n",
    "    (pump_train_df['spectral_bandwidth'] < limit_spectral_bandwidth[0]) |\n",
    "    (pump_train_df['spectral_bandwidth'] > limit_spectral_bandwidth[1])\n",
    ")[0]\n",
    "\n",
    "# Collect all abnormal indices\n",
    "all_abnormalities_train = np.unique(np.concatenate((\n",
    "    abn_num_peaks,\n",
    "    abn_spectral_entropy,\n",
    "    abn_spectral_centroid_mean,\n",
    "    abn_spectral_centroid_std,\n",
    "    abn_energy_ratio,\n",
    "    abn_std_spectrogram,\n",
    "    abn_peak_2000,\n",
    "    abn_spectral_bandwidth\n",
    ")))\n",
    "\n",
    "# Compare sets together\n",
    "pump_train_dict = {\n",
    "    \"Num Peaks\": abn_num_peaks,\n",
    "    \"Spectral Entropy\": abn_spectral_entropy,\n",
    "    \"Spectral Centroid Mean\": abn_spectral_centroid_mean,\n",
    "    \"Spectral Centroid Std\": abn_spectral_centroid_std,\n",
    "    \"Energy Ratio\": abn_energy_ratio,\n",
    "    \"Std Spectrogram\": abn_std_spectrogram,\n",
    "    \"Peak 2000 Hz Prominence\": abn_peak_2000,\n",
    "    \"Spectral Bandwidth\": abn_spectral_bandwidth\n",
    "}\n",
    "\n",
    "# Print anomalies found\n",
    "print(\"PUMP TRAINING DATASET\")\n",
    "for feature_name, anomalies in pump_train_dict.items():\n",
    "    print(f\"{feature_name}: {len(anomalies)} anomalies found.\")\n",
    "\n",
    "# Counting how many methods flagged each sample\n",
    "anomalies_count_train = np.array(\n",
    "    [np.sum([i in pump_train_dict[k] for k in pump_train_dict]) for i in range(pump_train_df.shape[0])]\n",
    ")\n",
    "\n",
    "# Print counts of samples flagged as anomalous by at least i selection methods\n",
    "for i in range(1, 8):\n",
    "    print(f\"{len(np.where(anomalies_count_train >= i)[0])} samples are found as anomalous by at least {i} selection methods.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e965b9ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect anomalies in the test dataset\n",
    "abn_num_peaks_test = np.where(\n",
    "    (pump_test_df['num_peaks'] < limit_num_peaks[0]) |\n",
    "    (pump_test_df['num_peaks'] > limit_num_peaks[1])\n",
    ")[0]\n",
    "\n",
    "abn_spectral_entropy_test = np.where(\n",
    "    (pump_test_df['spectral_entropy_mean'] < limit_spectral_entropy[0]) |\n",
    "    (pump_test_df['spectral_entropy_mean'] > limit_spectral_entropy[1])\n",
    ")[0]\n",
    "\n",
    "abn_spectral_centroid_mean_test = np.where(\n",
    "    (pump_test_df['spectral_centroid_mean'] < limit_spectral_centroid_mean[0]) |\n",
    "    (pump_test_df['spectral_centroid_mean'] > limit_spectral_centroid_mean[1])\n",
    ")[0]\n",
    "\n",
    "abn_spectral_centroid_std_test = np.where(\n",
    "    (pump_test_df['spectral_centroid_std'] < limit_spectral_centroid_std[0]) |\n",
    "    (pump_test_df['spectral_centroid_std'] > limit_spectral_centroid_std[1])\n",
    ")[0]\n",
    "\n",
    "abn_energy_ratio_test = np.where(\n",
    "    (pump_test_df['energy_ratio'] < limit_energy_ratio[0]) |\n",
    "    (pump_test_df['energy_ratio'] > limit_energy_ratio[1])\n",
    ")[0]\n",
    "\n",
    "abn_std_spectrogram_test = np.where(\n",
    "    (pump_test_df['std_spectogram'] < limit_std_spectogram[0]) |\n",
    "    (pump_test_df['std_spectogram'] > limit_std_spectogram[1])\n",
    ")[0]\n",
    "\n",
    "abn_peak_2000_test = np.where(\n",
    "    (pump_test_df['peak_2000_prominence'] < limit_peak_2000[0]) |\n",
    "    (pump_test_df['peak_2000_prominence'] > limit_peak_2000[1])\n",
    ")[0]\n",
    "\n",
    "abn_spectral_bandwidth = np.where(\n",
    "    (pump_test_df['spectral_bandwidth'] < limit_spectral_bandwidth[0]) |\n",
    "    (pump_test_df['spectral_bandwidth'] > limit_spectral_bandwidth[1])\n",
    ")[0]\n",
    "\n",
    "# Collect all abnormal indices\n",
    "all_abnormalities_test = np.unique(np.concatenate((\n",
    "    abn_num_peaks_test,\n",
    "    abn_spectral_entropy_test,\n",
    "    abn_spectral_centroid_mean_test,\n",
    "    abn_spectral_centroid_std_test,\n",
    "    abn_energy_ratio_test,\n",
    "    abn_std_spectrogram_test,\n",
    "    abn_peak_2000_test,\n",
    "    abn_spectral_bandwidth\n",
    ")))\n",
    "\n",
    "# Compare sets together\n",
    "pump_test_dict = {\n",
    "    \"Num Peaks\": abn_num_peaks_test,\n",
    "    \"Spectral Entropy\": abn_spectral_entropy_test,\n",
    "    \"Spectral Centroid Mean\": abn_spectral_centroid_mean_test,\n",
    "    \"Spectral Centroid Std\": abn_spectral_centroid_std_test,\n",
    "    \"Energy Ratio\": abn_energy_ratio_test,\n",
    "    \"Std Spectrogram\": abn_std_spectrogram_test,\n",
    "    \"Peak 2000 Hz Prominence\": abn_peak_2000_test,\n",
    "    \"Spectral Bandwidth\": abn_spectral_bandwidth\n",
    "}\n",
    "\n",
    "# Print anomalies found\n",
    "print(\"PUMP TEST DATASET\")\n",
    "for feature_name, anomalies in pump_test_dict.items():\n",
    "    print(f\"{feature_name}: {len(anomalies)} anomalies found.\")\n",
    "\n",
    "# Counting how many methods flagged each sample\n",
    "anomalies_count_test = np.array(\n",
    "    [np.sum([i in pump_test_dict[k] for k in pump_test_dict]) for i in range(pump_test_df.shape[0])]\n",
    ")\n",
    "\n",
    "# Print counts of samples flagged as anomalous by at least i selection methods\n",
    "for i in range(1, 8):\n",
    "    print(f\"{len(np.where(anomalies_count_test >= i)[0])} samples are found as anomalous by at least {i} selection methods.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cea8daad",
   "metadata": {},
   "outputs": [],
   "source": [
    "quantiles = [0.2, 99.8]\n",
    "limit_mean = np.percentile(valve_train_moments.loc[:, \"Mean\"], quantiles)\n",
    "limit_variance = np.percentile(valve_train_moments.loc[:, \"Variance\"], quantiles)\n",
    "lim_skew = np.percentile(valve_train_moments.loc[:, \"Skewness\"], quantiles)\n",
    "limit_kurtosis = np.percentile(valve_train_moments.loc[:, \"Kurtosis\"], quantiles)\n",
    "plt.figure(3, figsize=(15, 8))\n",
    "plt.clf()\n",
    "plt.suptitle(\n",
    "    \"Venn diagram between recordings considered abnormals by features 'Mean' and 'Variance'.\",\n",
    "    fontsize=20,\n",
    ")\n",
    "\n",
    "loc_train_mean = (valve_train_moments.loc[:, \"Mean\"] < limit_mean[0]) | (\n",
    "    valve_train_moments.loc[:, \"Mean\"] > limit_mean[1]\n",
    ")\n",
    "loc_train_variance = (valve_train_moments.loc[:, \"Variance\"] < limit_variance[0]) | (\n",
    "    valve_train_moments.loc[:, \"Variance\"] > limit_variance[1]\n",
    ")\n",
    "a1 = np.sum(loc_train_mean & loc_train_variance)\n",
    "a2 = np.sum(loc_train_mean & ~loc_train_variance)\n",
    "a3 = np.sum(~loc_train_mean & loc_train_variance)\n",
    "\n",
    "plt.subplot(121)\n",
    "venn2(subsets=(a2, a3, a1), set_labels=(\"Mean\", \"Variance\"))\n",
    "plt.title(\"Training dataset\")\n",
    "\n",
    "\n",
    "loc_test_mean = (valve_test_moments.loc[:, \"Mean\"] < limit_mean[0]) | (\n",
    "    valve_test_moments.loc[:, \"Mean\"] > limit_mean[1]\n",
    ")\n",
    "loc_test_variance = (valve_test_moments.loc[:, \"Variance\"] < limit_variance[0]) | (\n",
    "    valve_test_moments.loc[:, \"Variance\"] > limit_variance[1]\n",
    ")\n",
    "a1 = np.sum(loc_test_mean & loc_test_variance)\n",
    "a2 = np.sum(loc_test_mean & ~loc_test_variance)\n",
    "a3 = np.sum(~loc_test_mean & loc_test_variance)\n",
    "\n",
    "\n",
    "plt.subplot(122)\n",
    "venn2(subsets=(a2, a3, a1), set_labels=(\"Mean\", \"Variance\"))\n",
    "plt.title(\"Test dataset\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e55d60c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "quantiles = [0.2, 99.8]\n",
    "limit_mean = np.percentile(valve_train_moments.loc[:, \"Mean\"], quantiles)\n",
    "limit_variance = np.percentile(valve_train_moments.loc[:, \"Variance\"], quantiles)\n",
    "lim_skew = np.percentile(valve_train_moments.loc[:, \"Skewness\"], quantiles)\n",
    "limit_kurtosis = np.percentile(valve_train_moments.loc[:, \"Kurtosis\"], quantiles)\n",
    "plt.figure(3, figsize=(15, 8))\n",
    "plt.clf()\n",
    "plt.suptitle(\n",
    "    \"Venn diagram between recordings considered abnormals by features 'kurtosis' and 'Variance'.\",\n",
    "    fontsize=20,\n",
    ")\n",
    "\n",
    "loc_train_mean = (valve_train_moments.loc[:, \"Kurtosis\"] < limit_kurtosis[0]) | (\n",
    "    valve_train_moments.loc[:, \"Kurtosis\"] > limit_kurtosis[1]\n",
    ")\n",
    "loc_train_variance = (valve_train_moments.loc[:, \"Variance\"] < limit_variance[0]) | (\n",
    "    valve_train_moments.loc[:, \"Variance\"] > limit_variance[1]\n",
    ")\n",
    "a1 = np.sum(loc_train_mean & loc_train_variance)\n",
    "a2 = np.sum(loc_train_mean & ~loc_train_variance)\n",
    "a3 = np.sum(~loc_train_mean & loc_train_variance)\n",
    "\n",
    "plt.subplot(121)\n",
    "venn2(subsets=(a2, a3, a1), set_labels=(\"Kurtosis\", \"Variance\"))\n",
    "plt.title(\"Training dataset\")\n",
    "\n",
    "\n",
    "loc_test_mean = (valve_test_moments.loc[:, \"Kurtosis\"] < limit_kurtosis[0]) | (\n",
    "    valve_test_moments.loc[:, \"Kurtosis\"] > limit_kurtosis[1]\n",
    ")\n",
    "loc_test_variance = (valve_test_moments.loc[:, \"Variance\"] < limit_variance[0]) | (\n",
    "    valve_test_moments.loc[:, \"Variance\"] > limit_variance[1]\n",
    ")\n",
    "a1 = np.sum(loc_test_mean & loc_test_variance)\n",
    "a2 = np.sum(loc_test_mean & ~loc_test_variance)\n",
    "a3 = np.sum(~loc_test_mean & loc_test_variance)\n",
    "\n",
    "\n",
    "plt.subplot(122)\n",
    "venn2(subsets=(a2, a3, a1), set_labels=(\"kurtosis\", \"Variance\"))\n",
    "plt.title(\"Test dataset\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "391eedac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the quantiles for threshold lines\n",
    "quantiles = [0.2, 99.8]\n",
    "\n",
    "# Calculate limits for each feature\n",
    "limit_num_peaks = np.percentile(pump_train_df['num_peaks'], quantiles)\n",
    "limit_spectral_entropy = np.percentile(pump_train_df['spectral_entropy_mean'], quantiles)\n",
    "limit_spectral_centroid_mean = np.percentile(pump_train_df['spectral_centroid_mean'], quantiles)\n",
    "limit_spectral_centroid_std = np.percentile(pump_train_df['spectral_centroid_std'], quantiles)\n",
    "limit_energy_ratio = np.percentile(pump_train_df['energy_ratio'], quantiles)\n",
    "limit_std_spectogram = np.percentile(pump_train_df['std_spectogram'], quantiles)\n",
    "limit_peak_2000 = np.percentile(pump_train_df['peak_2000_prominence'], quantiles)\n",
    "limit_spectral_bandwidth = np.percentile(pump_train_df['spectral_bandwidth'], quantiles)\n",
    "\n",
    "plt.figure(3, figsize=(15, 8))\n",
    "plt.clf()\n",
    "plt.suptitle(\n",
    "    \"Venn diagram between recordings considered abnormals by features 'peak_2000_prominence' and 'Variance'.\",\n",
    "    fontsize=20,\n",
    ")\n",
    "\n",
    "loc_train_mean = (pump_train_df.loc[:, \"peak_2000_prominence\"] < limit_peak_2000[0]) | (\n",
    "    pump_train_df.loc[:, \"peak_2000_prominence\"] > limit_peak_2000[1]\n",
    ")\n",
    "loc_train_variance = (pump_train_df.loc[:, \"spectral_centroid_mean\"] < limit_spectral_centroid_mean[0]) | (\n",
    "    pump_train_df.loc[:, \"spectral_centroid_mean\"] > limit_spectral_centroid_mean[1]\n",
    ")\n",
    "a1 = np.sum(loc_train_mean & loc_train_variance)\n",
    "a2 = np.sum(loc_train_mean & ~loc_train_variance)\n",
    "a3 = np.sum(~loc_train_mean & loc_train_variance)\n",
    "\n",
    "plt.subplot(121)\n",
    "venn2(subsets=(a2, a3, a1), set_labels=(\"peak_2000_prominence\", \"spectral_centroid_mean\"))\n",
    "plt.title(\"Training dataset\")\n",
    "\n",
    "\n",
    "loc_test_mean = (pump_test_df.loc[:, \"peak_2000_prominence\"] < limit_peak_2000[0]) | (\n",
    "    pump_test_df.loc[:, \"peak_2000_prominence\"] > limit_peak_2000[1]\n",
    ")\n",
    "loc_test_variance = (pump_test_df.loc[:, \"spectral_centroid_mean\"] < limit_spectral_centroid_mean[0]) | (\n",
    "    pump_test_df.loc[:, \"spectral_centroid_mean\"] > limit_spectral_centroid_mean[1]\n",
    ")\n",
    "a1 = np.sum(loc_test_mean & loc_test_variance)\n",
    "a2 = np.sum(loc_test_mean & ~loc_test_variance)\n",
    "a3 = np.sum(~loc_test_mean & loc_test_variance)\n",
    "\n",
    "\n",
    "plt.subplot(122)\n",
    "venn2(subsets=(a2, a3, a1), set_labels=(\"peak_2000_prominence\", \"spectral_centroid_mean\"))\n",
    "plt.title(\"Test dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36693cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(3, figsize=(15, 8))\n",
    "plt.clf()\n",
    "plt.suptitle(\n",
    "    \"Venn diagram between recordings considered abnormals by features 'peak_2000_prominence' and 'Variance'.\",\n",
    "    fontsize=20,\n",
    ")\n",
    "\n",
    "loc_train_mean = (pump_train_df.loc[:, \"peak_2000_prominence\"] < limit_peak_2000[0]) | (\n",
    "    pump_train_df.loc[:, \"peak_2000_prominence\"] > limit_peak_2000[1]\n",
    ")\n",
    "loc_train_variance = (pump_train_df.loc[:, \"spectral_bandwidth\"] < limit_spectral_bandwidth[0]) | (\n",
    "    pump_train_df.loc[:, \"spectral_bandwidth\"] > limit_spectral_bandwidth[1]\n",
    ")\n",
    "a1 = np.sum(loc_train_mean & loc_train_variance)\n",
    "a2 = np.sum(loc_train_mean & ~loc_train_variance)\n",
    "a3 = np.sum(~loc_train_mean & loc_train_variance)\n",
    "\n",
    "plt.subplot(121)\n",
    "venn2(subsets=(a2, a3, a1), set_labels=(\"peak_2000_prominence\", \"spectral_bandwidth\"))\n",
    "plt.title(\"Training dataset\")\n",
    "\n",
    "\n",
    "loc_test_mean = (pump_test_df.loc[:, \"peak_2000_prominence\"] < limit_peak_2000[0]) | (\n",
    "    pump_test_df.loc[:, \"peak_2000_prominence\"] > limit_peak_2000[1]\n",
    ")\n",
    "loc_test_variance = (pump_test_df.loc[:, \"spectral_bandwidth\"] < limit_spectral_bandwidth[0]) | (\n",
    "    pump_test_df.loc[:, \"spectral_bandwidth\"] > limit_spectral_bandwidth[1]\n",
    ")\n",
    "a1 = np.sum(loc_test_mean & loc_test_variance)\n",
    "a2 = np.sum(loc_test_mean & ~loc_test_variance)\n",
    "a3 = np.sum(~loc_test_mean & loc_test_variance)\n",
    "\n",
    "\n",
    "plt.subplot(122)\n",
    "venn2(subsets=(a2, a3, a1), set_labels=(\"peak_2000_prominence\", \"spectral_bandwidth\"))\n",
    "plt.title(\"Test dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a8c32cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(3, figsize=(15, 8))\n",
    "plt.clf()\n",
    "plt.suptitle(\n",
    "    \"Venn diagram between recordings considered abnormals by features 'peak_2000_prominence' and 'Variance'.\",\n",
    "    fontsize=20,\n",
    ")\n",
    "\n",
    "loc_train_mean = (pump_train_df.loc[:, \"spectral_centroid_mean\"] < limit_spectral_centroid_mean[0]) | (\n",
    "    pump_train_df.loc[:, \"spectral_centroid_mean\"] > limit_spectral_centroid_mean[1]\n",
    ")\n",
    "loc_train_variance = (pump_train_df.loc[:, \"spectral_bandwidth\"] < limit_spectral_bandwidth[0]) | (\n",
    "    pump_train_df.loc[:, \"spectral_bandwidth\"] > limit_spectral_bandwidth[1]\n",
    ")\n",
    "a1 = np.sum(loc_train_mean & loc_train_variance)\n",
    "a2 = np.sum(loc_train_mean & ~loc_train_variance)\n",
    "a3 = np.sum(~loc_train_mean & loc_train_variance)\n",
    "\n",
    "plt.subplot(121)\n",
    "venn2(subsets=(a2, a3, a1), set_labels=(\"spectral_centroid_mean\", \"spectral_bandwidth\"))\n",
    "plt.title(\"Training dataset\")\n",
    "\n",
    "\n",
    "loc_test_mean = (pump_test_df.loc[:, \"spectral_centroid_mean\"] < limit_spectral_centroid_mean[0]) | (\n",
    "    pump_test_df.loc[:, \"spectral_centroid_mean\"] > limit_spectral_centroid_mean[1]\n",
    ")\n",
    "loc_test_variance = (pump_test_df.loc[:, \"spectral_bandwidth\"] < limit_spectral_bandwidth[0]) | (\n",
    "    pump_test_df.loc[:, \"spectral_bandwidth\"] > limit_spectral_bandwidth[1]\n",
    ")\n",
    "a1 = np.sum(loc_test_mean & loc_test_variance)\n",
    "a2 = np.sum(loc_test_mean & ~loc_test_variance)\n",
    "a3 = np.sum(~loc_test_mean & loc_test_variance)\n",
    "\n",
    "\n",
    "plt.subplot(122)\n",
    "venn2(subsets=(a2, a3, a1), set_labels=(\"spectral_centroid_mean\", \"spectral_bandwidth\"))\n",
    "plt.title(\"Test dataset\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
